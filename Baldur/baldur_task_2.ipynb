{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from task2_funcs import *\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Consultancy for opening a restaurant in the city of Philadelphia\n",
    "\n",
    "<!-- Task 2 is more open in nature as there is no specific target. We don’t expect you to analyse all aspects of the problem. You can decide yourself on which approaches, summary statistics or analysis procedures you focus on. Grading will be based on scientific correctness, originality and presentation. -->\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this report is to analyse customer review data to better understand the regional market so that the business is more likely to achieve success for the opening of their restaurant in Philadelphia. In particular, we aim to answer the following questions:\n",
    "\n",
    "1. An insight on what restaurant consumers generally seem to like (for example in terms of food, service, location, etc…).\n",
    "2. An analysis of the evolution of food trends in the area over time, in terms of consumer preferences. Do the preferences evolve over time, or do they seem stable?\n",
    "3. Imagine you have to present your findings to the business owner and his investors. What advice would you give to the new business, based on your findings?\n",
    "\n",
    "Given the nature of the task, we'll focus on customer reviews which concern restaurants located in the city of Philadelphia. First, let's take a look at the data to see how we can approach this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting restaurant reviews in Philadelphia\n",
    "Before starting the analysis, we'll have to obtain the relevant review data. Let's start by importing the data and then inspecting their structures. It is worth mentioning that the reviews in the test data (`ATML2024_reviews_test.csv`) are NOT used in this analysis, since they don't contain the ratings by customers for the businesses which will hinder our ability to know the preferences of restaurants in Philadelphia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "reviews_df = pd.read_csv(\"datasets/ATML2024_reviews_train.csv\")\n",
    "users_df = pd.read_csv(\"datasets/ATML2024_users.csv\")\n",
    "business_df = pd.read_csv(\"datasets/ATML2024_businesses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glimpsing at the data\n",
    "Below cells show the data types of the columns as well as the first 5 rows of each dataset. Based on the output, in order to extract reviews about restaurants in Philadelphia, we can first filter out businesses who are based in Philadelphia under the `city` column, and then look at those whose categories include restaurants. One crucial thing to note that, however, is that due to the textual nature of the data, there's no guarantee that the `city` column is free of typos or has standardised how Philadelphia is referred to. For example, the city is sometimes referred to as Philly. We shall inspect this column more in detail to ensure that we include all the restaurant reviews in Philadelphia (or at least we don't miss out too much because of typos).\n",
    "\n",
    "We can also notice that some columns aren't in the correct data types and will need to be changed if they're to be used in the following analysis. For instance, the date-related columns (`date` in `reviews_df` and `user_since` in `users_df`) are wrongly marked as `object`, and the `premium_account` column is just a string of years concatenated together which might pose some troubles if we'd like to look like the number of premium users by year. But for now let's focus on filtering restaurant reviews in Philadelphia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1050000 entries, 0 to 1049999\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   id           1050000 non-null  int64 \n",
      " 1   user_id      1050000 non-null  object\n",
      " 2   business_id  1050000 non-null  object\n",
      " 3   rating       1050000 non-null  int64 \n",
      " 4   useful       1050000 non-null  int64 \n",
      " 5   funny        1050000 non-null  int64 \n",
      " 6   cool         1050000 non-null  int64 \n",
      " 7   text         1050000 non-null  object\n",
      " 8   date         1050000 non-null  object\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 72.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(reviews_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   id | user_id                | business_id            |   rating |   useful |   funny |   cool | text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | date                |\n",
      "|------+------------------------+------------------------+----------+----------+---------+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------|\n",
      "|    0 | gRWsscMiClFGIh2YUjNInT | Ab1ejQzv4eJRLgAlj7tzKT |        5 |        1 |       0 |      0 | Sometimes you just want a cup of coffee, in a quiet place, with no conversation, loud music, BS, etc. and Portland Brew is it.  I would normally go to Frothy, but it's normally so freaking loud and full of  Belmont kids, Portland is now my default.  I also appreciate that most of the time, when i get my normal iced coffee with soy, that I'm given the soy to pour my preferred amount.  It's the little things, i guess.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 2013-12-22 23:45:01 |\n",
      "|      |                        |                        |          |          |         |        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                     |\n",
      "|      |                        |                        |          |          |         |        | Thumbs up.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                     |\n",
      "|    1 | Q3RXWVFtLo4I7MC85ELPh7 | QkRhOowLAGVZC5gn4RLkjH |        5 |        1 |       0 |      0 | Mr. Clyde stopped at our house and asked us if we had any tree removal work for him. We said that we had a huge, dying/dead palm tree and needed it removed. Clyde and Sons Tree Service was working at a neighbor's house, so we saw that they were legitimate. They came back to our house the next week with their equipment and performed the job at the time and price we negotiated. He climbed up the tree and did an excellent, careful job and removed the 4500 lb.tree which was quite close to our house. They also ground up the tree stump, which was double in size, because half of the tree was already cut down in years past, and the old stump was left next to the present tree. We felt his price was very reasonable for the extent of the job.  I would not hesitate to call Clyde and Sons Tree Service for removal of any tree and stump. Very competent, hardworking gentlemen. Thank you guys! | 2020-02-11 01:00:32 |\n",
      "|    2 | AD2z6qhmVcRJkywa1KsEFG | ADWVkizi8gKjj07mn8NiMe |        4 |        0 |       0 |      0 | One of my favorite places to grab a quick bit to eat. I have only been to locations in the airport and stadiums.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 2014-05-28 19:05:26 |\n",
      "|      |                        |                        |          |          |         |        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                     |\n",
      "|      |                        |                        |          |          |         |        | The crab fries are one of the most amazing things you will ever try. The wiz is litteraly liquid gold (be sure to buy an extra one).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                     |\n",
      "|      |                        |                        |          |          |         |        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                     |\n",
      "|      |                        |                        |          |          |         |        | Beware of the other food on the menu... it is typically just greasy bar food. Stick to the crab fries and you will be golden!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                     |\n",
      "|    3 | A8HQot2boCpoPIThfHV5iy | wkk30dWY49cSiDU3FKg8FH |        5 |        0 |       1 |      0 | Wow, this place is huge! Life size aquarium and Ferris wheel.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 2014-01-20 23:42:44 |\n",
      "|      |                        |                        |          |          |         |        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                     |\n",
      "|      |                        |                        |          |          |         |        | Also all the sports departments are individualized and the customer service is great.  Was asked three different times for help when looking at golf balls.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                     |\n",
      "|      |                        |                        |          |          |         |        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                     |\n",
      "|      |                        |                        |          |          |         |        | This is the place for sports, not to mention all the guns they have.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                     |\n",
      "|    4 | Q7UzjD01YnimUOsvAEQ7JK | wUjnssL0bJIEqdqJs8M0GV |        5 |        0 |       0 |      0 | I ordered 3 large pizzas and  wings for my family. The pizzas were HUGE and cooked to perfection. Best New York style pizza I've had in a long time. The mild and parmesan garlic wings were fantastic, lightly crisp with the right amount of sauce. My only disappointment was that I thought for sure we would have a ton of left overs but it was so delicious we ate almost everything. I can't wait for my next order!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2020-01-29 00:20:34 |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(reviews_df.head(), headers = \"keys\", tablefmt='orgtbl', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 747468 entries, 0 to 747467\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   user_id             747468 non-null  object \n",
      " 1   name                747457 non-null  object \n",
      " 2   user_since          747468 non-null  object \n",
      " 3   useful              747468 non-null  float64\n",
      " 4   funny               747468 non-null  float64\n",
      " 5   cool                747468 non-null  float64\n",
      " 6   premium_account     57420 non-null   object \n",
      " 7   friends             747468 non-null  float64\n",
      " 8   fans                747468 non-null  float64\n",
      " 9   compliment_hot      747468 non-null  float64\n",
      " 10  compliment_more     747468 non-null  float64\n",
      " 11  compliment_profile  747468 non-null  float64\n",
      " 12  compliment_cute     747468 non-null  float64\n",
      " 13  compliment_list     747468 non-null  float64\n",
      " 14  compliment_note     747468 non-null  float64\n",
      " 15  compliment_plain    747468 non-null  float64\n",
      " 16  compliment_cool     747468 non-null  float64\n",
      " 17  compliment_funny    747468 non-null  float64\n",
      " 18  compliment_writer   747468 non-null  float64\n",
      "dtypes: float64(15), object(4)\n",
      "memory usage: 108.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(users_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| user_id                | name   | user_since          |   useful |   funny |   cool | premium_account                                                   |   friends |   fans |   compliment_hot |   compliment_more |   compliment_profile |   compliment_cute |   compliment_list |   compliment_note |   compliment_plain |   compliment_cool |   compliment_funny |   compliment_writer |\n",
      "|------------------------+--------+---------------------+----------+---------+--------+-------------------------------------------------------------------+-----------+--------+------------------+-------------------+----------------------+-------------------+-------------------+-------------------+--------------------+-------------------+--------------------+---------------------|\n",
      "| w7IdXgBVXKjZS5UYDO8cVq | Walker | 2007-01-25 16:47:26 |     7217 |    1259 |   5994 | 2007                                                              |     14995 |    267 |              250 |                65 |                   55 |                56 |                18 |               232 |                844 |               467 |                467 |                 239 |\n",
      "| gJrXd1wa1EZ2-_UoRgW41j | Daniel | 2009-01-25 04:35:42 |    43091 |   13066 |  27281 | 2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,20,20,2021 |      4646 |   3138 |             1145 |               264 |                  184 |               157 |               251 |              1847 |               7054 |              3131 |               3131 |                1521 |\n",
      "| gvz2VtPxToEXh0KFQYXnW2 | Steph  | 2008-07-25 10:41:00 |     2086 |    1010 |   1003 | 2009,2010,2011,2012,2013                                          |       381 |     52 |               89 |                13 |                   10 |                17 |                 3 |                66 |                 96 |               119 |                119 |                  35 |\n",
      "| AIdshsLNMm50o7qXSAeDZS | Gwen   | 2005-11-29 04:38:33 |      512 |     330 |    299 | 2009,2010,2011                                                    |       131 |     28 |               24 |                 4 |                    1 |                 6 |                 2 |                12 |                 16 |                26 |                 26 |                  10 |\n",
      "| QGFh-RoJ4HscnnE-yMl5Ah | Karen  | 2007-01-05 19:40:59 |       29 |      15 |      7 | nan                                                               |        27 |      1 |                1 |                 1 |                    0 |                 0 |                 0 |                 1 |                  1 |                 0 |                  0 |                   0 |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(users_df.head(), headers='keys', tablefmt='orgtbl', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138210 entries, 0 to 138209\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   business_id  138210 non-null  object \n",
      " 1   name         138210 non-null  object \n",
      " 2   address      133772 non-null  object \n",
      " 3   city         138210 non-null  object \n",
      " 4   state        138210 non-null  object \n",
      " 5   postal_code  138145 non-null  object \n",
      " 6   latitude     138210 non-null  float64\n",
      " 7   longitude    138210 non-null  float64\n",
      " 8   attributes   126589 non-null  object \n",
      " 9   categories   138136 non-null  object \n",
      " 10  hours        117852 non-null  object \n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 11.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(business_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| business_id            | name                     | address                         | city         | state   |   postal_code |   latitude |   longitude | attributes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | categories                                                                         | hours                                                                                                                                                              |\n",
      "|------------------------+--------------------------+---------------------------------+--------------+---------+---------------+------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| wPYArZCy3AETdTjB-x3fpm | The UPS Store            | 87 Grasso Plaza Shopping Center | Affton       | MO      |         63123 |    38.5511 |    -90.3357 | {'BusinessAcceptsCreditCards': 'True'}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Shipping Centers, Local Services, Notaries, Mailbox Centers, Printing Services     | {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', 'Wednesday': '8:0-18:30', 'Thursday': '8:0-18:30', 'Friday': '8:0-18:30', 'Saturday': '8:0-14:0'}                    |\n",
      "| QQNIWVsnAT_iKiKriWrFUt | Target                   | 5255 E Broadway Blvd            | Tucson       | AZ      |         85711 |    32.2232 |   -110.88   | {'BikeParking': 'True', 'BusinessAcceptsCreditCards': 'True', 'RestaurantsPriceRange2': '2', 'CoatCheck': 'False', 'RestaurantsTakeOut': 'False', 'RestaurantsDelivery': 'False', 'Caters': 'False', 'WiFi': \"u'no'\", 'BusinessParking': \"{'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}\", 'WheelchairAccessible': 'True', 'HappyHour': 'False', 'OutdoorSeating': 'False', 'HasTV': 'False', 'RestaurantsReservations': 'False', 'DogsAllowed': 'False', 'ByAppointmentOnly': 'False'}                                                                                                               | Department Stores, Shopping, Fashion, Home & Garden, Electronics, Furniture Stores | {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', 'Wednesday': '8:0-22:0', 'Thursday': '8:0-22:0', 'Friday': '8:0-23:0', 'Saturday': '8:0-23:0', 'Sunday': '8:0-22:0'} |\n",
      "| wm9eoqjytVbC7dQcM4WSTM | St Honore Pastries       | 935 Race St                     | Philadelphia | PA      |         19107 |    39.9555 |    -75.1556 | {'RestaurantsDelivery': 'False', 'OutdoorSeating': 'False', 'BusinessAcceptsCreditCards': 'False', 'BusinessParking': \"{'garage': False, 'street': True, 'validated': False, 'lot': False, 'valet': False}\", 'BikeParking': 'True', 'RestaurantsPriceRange2': '1', 'RestaurantsTakeOut': 'True', 'ByAppointmentOnly': 'False', 'WiFi': \"u'free'\", 'Alcohol': \"u'none'\", 'Caters': 'True'}                                                                                                                                                                                                                                                  | Restaurants, Food, Bubble Tea, Coffee & Tea, Bakeries                              | {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', 'Wednesday': '7:0-20:0', 'Thursday': '7:0-20:0', 'Friday': '7:0-21:0', 'Saturday': '7:0-21:0', 'Sunday': '7:0-21:0'} |\n",
      "| AfVDXGIKBUE0EdTw_6cMWm | Perkiomen Valley Brewery | 101 Walnut St                   | Green Lane   | PA      |         18054 |    40.3382 |    -75.4717 | {'BusinessAcceptsCreditCards': 'True', 'WheelchairAccessible': 'True', 'RestaurantsTakeOut': 'True', 'BusinessParking': \"{'garage': None, 'street': None, 'validated': None, 'lot': True, 'valet': False}\", 'BikeParking': 'True', 'GoodForKids': 'True', 'Caters': 'False'}                                                                                                                                                                                                                                                                                                                                                               | Brewpubs, Breweries, Food                                                          | {'Wednesday': '14:0-22:0', 'Thursday': '16:0-22:0', 'Friday': '12:0-22:0', 'Saturday': '12:0-22:0', 'Sunday': '12:0-18:0'}                                         |\n",
      "| QjvanH64QUduo6E-8F33FC | Sonic Drive-In           | 615 S Main St                   | Ashland City | TN      |         37015 |    36.2696 |    -87.0589 | {'BusinessParking': 'None', 'BusinessAcceptsCreditCards': 'True', 'RestaurantsAttire': \"u'casual'\", 'OutdoorSeating': 'True', 'RestaurantsReservations': 'False', 'Caters': 'False', 'RestaurantsTakeOut': 'True', 'Alcohol': \"u'none'\", 'Ambience': 'None', 'GoodForKids': 'True', 'RestaurantsPriceRange2': '1', 'ByAppointmentOnly': 'False', 'CoatCheck': 'False', 'DogsAllowed': 'False', 'RestaurantsTableService': 'False', 'RestaurantsGoodForGroups': 'True', 'RestaurantsDelivery': 'True', 'WiFi': \"u'no'\", 'WheelchairAccessible': 'True', 'HasTV': 'True', 'HappyHour': 'False', 'DriveThru': 'True', 'BikeParking': 'False'} | Burgers, Fast Food, Sandwiches, Food, Ice Cream & Frozen Yogurt, Restaurants       | {'Monday': '0:0-0:0', 'Tuesday': '6:0-22:0', 'Wednesday': '6:0-22:0', 'Thursday': '6:0-22:0', 'Friday': '9:0-0:0', 'Saturday': '9:0-22:0', 'Sunday': '8:0-22:0'}   |\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(business_df.head(), headers = \"keys\", tablefmt='orgtbl', showindex = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is it Philadelphia, or ...?\n",
    "We now take a closer look to how Philadelphia might be referred to in the dataset. From the regex search result below, even just by using the full name of the city, there already exist 6 ways to which the city is referred. Moreover, the below list doesn't include typos nor or nicknames of the city. Therefore, we'll need to normalise the different ways Philadelphia is called in the `business_df` dataset, and we need a way to deal with a non-exhaustive list of alias of Philadelphia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['philadelphia' 'southwest philadelphia' 'philadelphia pa'\n",
      " 'west philadelphia' 'philadelphia (northeast philly)' 'philadelphia ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Checking how Philadelphia might be referred to\n",
    "business_pa = business_df.query(\"state == 'PA'\")  # Since Philadelphia is located in the state of Pennsylvania\n",
    "business_pa.loc[:, 'city'] = business_pa['city'].str.lower()  # Avoid case-sensitivity issues in string matching later\n",
    "unique_cities = business_pa['city'].unique()  \n",
    "philly_matches = [re.search(r\"philadelphia\", city) is not None for city in unique_cities]\n",
    "print(unique_cities[philly_matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare similarities between some strings and \"Philadelphia\", we can use the Jaro similarity which ranges from 0 (totally dissimilar) to 1 (exact match) between two strings. Mathematically, Jaro similarity $sim_j$ between two strings $s_1 \\; \\text{and} \\; s_2$ is defined as below (a more detailed discussion of the Jaro similairty and its variants can be found [here](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance)):\n",
    "\n",
    "\\begin{align}\n",
    "sim_j = \n",
    "\\begin{cases}\n",
    "0 \\quad &\\text{if} \\; m = 0 \\\\\n",
    "\\frac{1}{3}(\\frac{m}{|s_1|} + \\frac{m}{|s_2|} + \\frac{m-t}{m}) \\; &\\text{otherwise}\n",
    "\\end{cases},\n",
    "\\end{align}\n",
    "\n",
    "where $|s_i|$ is the length of the string $s_i$, $m$ is the number of matching characters (characters in $s_1 \\; \\text{and} \\; s_2$ are matching only if they're at most $[\\frac{\\text{max}(|s_1|, |s_2|)}{2}] - 1$ characters apart), and $t$ is the number of transpositions (i.e. swapping the positions of two characters) which is calculated as number of matching characters not being in the correct order divided by two.\n",
    "\n",
    "Admittedly, there exist other string distance measures such as the Levenshtein distance as briefly mentioned in class. Nevertheless, we will use Jaro similarity because its output always ranges from 0 to 1 no matter the lengths of the two strings under comparison which makes thresholding easier. Moreover, Jaro similarity satisfies the mathematical definition of a distance metric, whereas its variant Jaro-Winkler similarity violates the triangle inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move on to testing how many cities will be replaced by \"philadelphia\" depending on the threshold of Jaro similarity score. The python package `jellyfish` has already implemented a function for calculating Jaro similarity between two strings, and we shall define a function for replacing city names in the dataset based on this. Note that we've converted the unique city names in the dataset to be in lowercases since the implementation in `jellyfish` is case-sensitive, i.e. same character in different cases will not be counted as matching characters.\n",
    "\n",
    "Starting with an arbitrary threshold of 0.6, we notice that matches with scores lower than 0.7 are wrongly replaced with 'philadelphia' while in reality they are genuinely referring to another locale in the state of Pennsylvania. We therefore should not include these matches in our final dataset. On the other hand, the algorithm manages to capture the 11 different alias of Philadelphia and replace them accordingly once the Jaro score is at least 0.7. Indeed, all the matching results seem to be about the city of Philadelphia, whether it be typos or nicknames of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philadelphia -> philadelphia, score: 1.00\n",
      "philadelphia  -> philadelphia, score: 0.97\n",
      "philiadelphia -> philadelphia, score: 0.97\n",
      "philadephia -> philadelphia, score: 0.97\n",
      "philadelphila  -> philadelphia, score: 0.95\n",
      "philadelphia pa -> philadelphia, score: 0.93\n",
      "philiidelphia -> philadelphia, score: 0.83\n",
      "west philadelphia -> philadelphia, score: 0.82\n",
      "phila -> philadelphia, score: 0.81\n",
      "philadelphia (northeast philly) -> philadelphia, score: 0.80\n",
      "philly -> philadelphia, score: 0.75\n",
      "southwest philadelphia -> philadelphia, score: 0.71\n",
      "phonixville -> philadelphia, score: 0.69\n",
      "upland -> philadelphia, score: 0.67\n",
      "pineville -> philadelphia, score: 0.67\n",
      "landsdale -> philadelphia, score: 0.67\n",
      "silverdale -> philadelphia, score: 0.64\n",
      "holland -> philadelphia, score: 0.64\n",
      "red hill -> philadelphia, score: 0.64\n",
      "hatfield -> philadelphia, score: 0.64\n",
      "ivyland -> philadelphia, score: 0.63\n",
      "paoli -> philadelphia, score: 0.63\n",
      "pipersville -> philadelphia, score: 0.63\n",
      "springfield -> philadelphia, score: 0.63\n",
      "lansdale -> philadelphia, score: 0.61\n",
      "parkside -> philadelphia, score: 0.61\n",
      "lederach -> philadelphia, score: 0.61\n",
      "landsale -> philadelphia, score: 0.61\n",
      "pennsylvania -> philadelphia, score: 0.61\n",
      "rushland -> philadelphia, score: 0.61\n",
      "hilltown -> philadelphia, score: 0.61\n",
      "gladwyne -> philadelphia, score: 0.61\n"
     ]
    }
   ],
   "source": [
    "jaro_replace_names(unique_cities, \"philadelphia\", 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 0.7 as a threshold for the Jaro score to replace different alias of Philadelphia for filtering the business dataset later. It seems that the algorithm worked correctly judging from the fact that the `assert` statement, which checks how many aliases of Philadelphia were reduced after normalising the city's name in `business_df`, did not return any errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the alias of Philadelphia in the dataset\n",
    "old_unique_cities_len = len(unique_cities)\n",
    "business_pa.loc[:, \"city\"] = business_pa['city'].apply(replace_name, new_name = 'philadelphia', threshold = 0.7)\n",
    "new_unique_cities_len = len(business_pa['city'].unique())\n",
    "\n",
    "# Checking we've normalised Philadelphia correctly\n",
    "assert old_unique_cities_len - new_unique_cities_len == 11  # There were 11 aliases for Philadelphia in the original business_df dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Restaurant-related Businesses\n",
    "We shall also pre-process the `categories` column to ensure that we do not miss out restaurant-related reviews due to typos. Using regex to look for words containing part of the spelling of restaurants (namely \"Rest\"), we see that more than one result were returned, of which 3 contain the term 'restaurant.' Again, we can use Jaro similarity to check whether restaurants are referred to with multiple ways due to typos.\n",
    "\n",
    "Contrary to the case of the city's name, \"restaurants\" do not have many variants of how it is referred to in `business_df`. Indeed, only the categories of *restaurants* and *pop-up restaurants* seem to be relevant for helping the business owners understand the regional market of the catering industry. Therefore, we can just keep these two categories in `business_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['restaurants', 'art restoration', 'pop-up restaurants',\n",
       "       'restaurant supplies', 'damage restoration', 'rest stops'],\n",
       "      dtype='<U35')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Getting all the distinct business categories \n",
    "business_pa.loc[:, \"categories\"] = business_pa[\"categories\"].str.lower()\n",
    "categories_split = chain(*list(business_pa[\"categories\"].dropna().str.split(r\",\\s+\", regex=True)))\n",
    "unique_categories = list(set(categories_split))\n",
    "\n",
    "# Checking which categories might contain part of the spelling of the word restaurant\n",
    "resto_matches = [re.search(r\"Rest.*\", cat, flags=re.IGNORECASE) is not None for cat in unique_categories]\n",
    "np.array(unique_categories)[resto_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurants -> restaurants, score: 1.00\n",
      "restaurant supplies -> restaurants, score: 0.86\n",
      "pop-up restaurants -> restaurants, score: 0.78\n",
      "resorts -> restaurants, score: 0.75\n",
      "austrian -> restaurants, score: 0.74\n",
      "arts & crafts -> restaurants, score: 0.74\n",
      "art space rentals -> restaurants, score: 0.73\n",
      "party bus rentals -> restaurants, score: 0.73\n",
      "rest stops -> restaurants, score: 0.72\n",
      "vegetarian -> restaurants, score: 0.72\n",
      "aerial tours -> restaurants, score: 0.71\n",
      "race tracks -> restaurants, score: 0.71\n",
      "real estate -> restaurants, score: 0.71\n",
      "shutters -> restaurants, score: 0.71\n",
      "southern -> restaurants, score: 0.71\n",
      "attraction farms -> restaurants, score: 0.70\n",
      "fire departments -> restaurants, score: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Checking how many categories can be considered as variants of restaurant-related businesses\n",
    "jaro_replace_names(unique_categories, \"restaurants\", threshold = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting restaurant review data in Philly\n",
    "After normalising the name of the city as well as finding out the business categories which correspond to restaurants, we can extract the data needed for subsequent analysis. We also dropped some columns which will not be useful to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting businesses located in Philadelphia\n",
    "business_philly = business_pa.query(\"city == 'philadelphia'\").reset_index(drop = True)\n",
    "\n",
    "# Getting only restaurant businesses in Philadelphia\n",
    "restaurant_masks = business_philly[\"categories\"].str.contains(\"restaurants\") | business_philly[\"categories\"].str.contains(\"pop-up restaurants\")\n",
    "restos_philly = business_philly.loc[restaurant_masks, :]\n",
    "\n",
    "# Finally, joining user reviews with restaurant information\n",
    "restos_philly = reviews_df.merge(restos_philly, how = \"inner\", on = \"business_id\")\n",
    "assert np.sum(restos_philly['city'] == 'philadelphia') == restos_philly.shape[0]\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "restos_philly.drop(columns = [\"id\", \"business_id\", \"user_id\", \"city\", \"state\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now inspect the restaurant review data to see if more cleaning needs to be done. The data type of the `date` column is defined as `object` which will hinder our ability to analyse changes in customer preferences over time later, so let's convert it into `datetime` type object. By looking at the first few rows of the data, we also notice that the `attributes` column is in json format. It will be good to convert them into something easier to use for subsequent analysis if needed, namely by removing the symbols specific to json format with regex.\n",
    "\n",
    "Missing values are also present in some columns of our data, and this is relatively the most serious for `hours`. Given the primary task is to make sense of customer preferences for restaurants in Philadelphia, opening hours are likely to be less informative than food style and service for deciding the optimal business strategy to open a new restaurant in the city. Accordingly, we will simply drop the `hours` column first, and then check how many rows will be omitted if we only include rows which contain non-null values in all the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103032 entries, 0 to 103031\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   rating       103032 non-null  int64  \n",
      " 1   useful       103032 non-null  int64  \n",
      " 2   funny        103032 non-null  int64  \n",
      " 3   cool         103032 non-null  int64  \n",
      " 4   text         103032 non-null  object \n",
      " 5   date         103032 non-null  object \n",
      " 6   name         103032 non-null  object \n",
      " 7   address      102899 non-null  object \n",
      " 8   postal_code  103027 non-null  object \n",
      " 9   latitude     103032 non-null  float64\n",
      " 10  longitude    103032 non-null  float64\n",
      " 11  attributes   102969 non-null  object \n",
      " 12  categories   103032 non-null  object \n",
      " 13  hours        99327 non-null   object \n",
      "dtypes: float64(2), int64(4), object(8)\n",
      "memory usage: 11.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One of my favorite places to grab a quick bit ...</td>\n",
       "      <td>2014-05-28 19:05:26</td>\n",
       "      <td>Chickie's &amp; Pete's</td>\n",
       "      <td>1526 Packer Ave</td>\n",
       "      <td>19145</td>\n",
       "      <td>39.911417</td>\n",
       "      <td>-75.174511</td>\n",
       "      <td>{'GoodForKids': 'True', 'Corkage': 'False', 'H...</td>\n",
       "      <td>seafood, nightlife, sports bars, bars, restaur...</td>\n",
       "      <td>{'Monday': '11:0-2:0', 'Tuesday': '11:0-2:0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I love this place! There's nothing like authen...</td>\n",
       "      <td>2013-05-28 16:52:23</td>\n",
       "      <td>Guacamole Mex-Grill</td>\n",
       "      <td>4612 Woodland Ave</td>\n",
       "      <td>19143</td>\n",
       "      <td>39.943971</td>\n",
       "      <td>-75.209914</td>\n",
       "      <td>{'Caters': 'False', 'RestaurantsAttire': \"u'ca...</td>\n",
       "      <td>mexican, restaurants, specialty food, ethnic f...</td>\n",
       "      <td>{'Monday': '11:0-20:0', 'Tuesday': '11:0-20:0'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Situated on Race street away from the main str...</td>\n",
       "      <td>2014-12-21 15:03:49</td>\n",
       "      <td>Shiao Lan Kung</td>\n",
       "      <td>930 Race St</td>\n",
       "      <td>19107</td>\n",
       "      <td>39.955247</td>\n",
       "      <td>-75.155409</td>\n",
       "      <td>{'RestaurantsGoodForGroups': 'True', 'Restaura...</td>\n",
       "      <td>noodles, seafood, restaurants, chinese</td>\n",
       "      <td>{'Tuesday': '15:0-2:0', 'Wednesday': '15:0-2:0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Outstanding venue, great band, and the food is...</td>\n",
       "      <td>2015-10-17 01:07:26</td>\n",
       "      <td>Amari's Restaurant</td>\n",
       "      <td>5037 Baltimore Ave</td>\n",
       "      <td>19143</td>\n",
       "      <td>39.947985</td>\n",
       "      <td>-75.224744</td>\n",
       "      <td>{'HasTV': 'True', 'RestaurantsAttire': \"u'casu...</td>\n",
       "      <td>soul food, american (new), breakfast &amp; brunch,...</td>\n",
       "      <td>{'Wednesday': '7:30-15:0', 'Thursday': '7:30-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>We loved our experience at Elwood.\\n\\nWhen you...</td>\n",
       "      <td>2019-05-04 12:24:05</td>\n",
       "      <td>Elwood</td>\n",
       "      <td>1007 Frankford Ave</td>\n",
       "      <td>19125</td>\n",
       "      <td>39.966404</td>\n",
       "      <td>-75.134227</td>\n",
       "      <td>{'RestaurantsReservations': 'True', 'OutdoorSe...</td>\n",
       "      <td>american (traditional), american (new), food, ...</td>\n",
       "      <td>{'Thursday': '17:0-22:0', 'Friday': '17:0-22:0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  useful  funny  cool  \\\n",
       "0       4       0      0     0   \n",
       "1       5       0      0     0   \n",
       "2       4       2      0     0   \n",
       "3       5       0      0     0   \n",
       "4       5       4      0     2   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  One of my favorite places to grab a quick bit ...  2014-05-28 19:05:26   \n",
       "1  I love this place! There's nothing like authen...  2013-05-28 16:52:23   \n",
       "2  Situated on Race street away from the main str...  2014-12-21 15:03:49   \n",
       "3  Outstanding venue, great band, and the food is...  2015-10-17 01:07:26   \n",
       "4  We loved our experience at Elwood.\\n\\nWhen you...  2019-05-04 12:24:05   \n",
       "\n",
       "                  name             address postal_code   latitude  longitude  \\\n",
       "0   Chickie's & Pete's     1526 Packer Ave       19145  39.911417 -75.174511   \n",
       "1  Guacamole Mex-Grill   4612 Woodland Ave       19143  39.943971 -75.209914   \n",
       "2       Shiao Lan Kung         930 Race St       19107  39.955247 -75.155409   \n",
       "3   Amari's Restaurant  5037 Baltimore Ave       19143  39.947985 -75.224744   \n",
       "4               Elwood  1007 Frankford Ave       19125  39.966404 -75.134227   \n",
       "\n",
       "                                          attributes  \\\n",
       "0  {'GoodForKids': 'True', 'Corkage': 'False', 'H...   \n",
       "1  {'Caters': 'False', 'RestaurantsAttire': \"u'ca...   \n",
       "2  {'RestaurantsGoodForGroups': 'True', 'Restaura...   \n",
       "3  {'HasTV': 'True', 'RestaurantsAttire': \"u'casu...   \n",
       "4  {'RestaurantsReservations': 'True', 'OutdoorSe...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  seafood, nightlife, sports bars, bars, restaur...   \n",
       "1  mexican, restaurants, specialty food, ethnic f...   \n",
       "2             noodles, seafood, restaurants, chinese   \n",
       "3  soul food, american (new), breakfast & brunch,...   \n",
       "4  american (traditional), american (new), food, ...   \n",
       "\n",
       "                                               hours  \n",
       "0  {'Monday': '11:0-2:0', 'Tuesday': '11:0-2:0', ...  \n",
       "1  {'Monday': '11:0-20:0', 'Tuesday': '11:0-20:0'...  \n",
       "2  {'Tuesday': '15:0-2:0', 'Wednesday': '15:0-2:0...  \n",
       "3  {'Wednesday': '7:30-15:0', 'Thursday': '7:30-1...  \n",
       "4  {'Thursday': '17:0-22:0', 'Friday': '17:0-22:0...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the data types of the columns\n",
    "print(restos_philly.info())\n",
    "restos_philly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GoodForKids: True',\n",
       " 'Corkage: False',\n",
       " 'HasTV: True',\n",
       " 'RestaurantsAttire: casual',\n",
       " 'RestaurantsTakeOut: True']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the date column into datetime data type for easier manipulation\n",
    "restos_philly[\"date\"] = pd.to_datetime(restos_philly['date']).dt.normalize()\n",
    "\n",
    "# Cleaning attributes\n",
    "restos_philly[\"attributes\"] = restos_philly[\"attributes\"].str.replace(r\"\\{|\\}|'|\\\"|\\bu\", \"\", regex=True)\n",
    "restos_philly[\"attributes\"] = restos_philly[\"attributes\"].str.split(\", \")\n",
    "restos_philly[\"attributes\"][0][:5]  # I'll leave it like this for now but do further pre-processing if you need to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the shape of the `restos_philly` dataset after firsting dropping the `hours` columns and then the rows containing any null values, the decrease in the number of observations is only less than 200 which is minimal, so we will just proceed with this approach for dealing with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 102836 entries, 0 to 103031\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   rating       102836 non-null  int64         \n",
      " 1   useful       102836 non-null  int64         \n",
      " 2   funny        102836 non-null  int64         \n",
      " 3   cool         102836 non-null  int64         \n",
      " 4   text         102836 non-null  object        \n",
      " 5   date         102836 non-null  datetime64[ns]\n",
      " 6   name         102836 non-null  object        \n",
      " 7   address      102836 non-null  object        \n",
      " 8   postal_code  102836 non-null  object        \n",
      " 9   latitude     102836 non-null  float64       \n",
      " 10  longitude    102836 non-null  float64       \n",
      " 11  attributes   102836 non-null  object        \n",
      " 12  categories   102836 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(4), object(6)\n",
      "memory usage: 11.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Dropping the hours column \n",
    "restos_philly.drop(columns=['hours'], inplace = True)\n",
    "\n",
    "# Checking how many obs we'll lose by dropping rows which contain missing values in any of the columns\n",
    "restos_philly.dropna(inplace = True)  \n",
    "restos_philly.info()  # The loss in observation is fewer than 200 rows which is minimal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking again at the `info()` of the dataset, every columns now has the correct data type, and the missing values are removed. Looking at the first 5 rows of the dataset, our pre-processing steps also seemed to work as intended. However, if we would like to use machine learning techniques (e.g. dimensionality reduction) on the textual data (namely, `text`, `categories` and potentially `attributes`), then we will need a way to convert them into numerical embeddings which can be understood by computers. We shall discuss this more when we analyse the data in later parts of this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One of my favorite places to grab a quick bit ...</td>\n",
       "      <td>2014-05-28</td>\n",
       "      <td>Chickie's &amp; Pete's</td>\n",
       "      <td>1526 Packer Ave</td>\n",
       "      <td>19145</td>\n",
       "      <td>39.911417</td>\n",
       "      <td>-75.174511</td>\n",
       "      <td>[GoodForKids: True, Corkage: False, HasTV: Tru...</td>\n",
       "      <td>seafood, nightlife, sports bars, bars, restaur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I love this place! There's nothing like authen...</td>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>Guacamole Mex-Grill</td>\n",
       "      <td>4612 Woodland Ave</td>\n",
       "      <td>19143</td>\n",
       "      <td>39.943971</td>\n",
       "      <td>-75.209914</td>\n",
       "      <td>[Caters: False, RestaurantsAttire: casual, Noi...</td>\n",
       "      <td>mexican, restaurants, specialty food, ethnic f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Situated on Race street away from the main str...</td>\n",
       "      <td>2014-12-21</td>\n",
       "      <td>Shiao Lan Kung</td>\n",
       "      <td>930 Race St</td>\n",
       "      <td>19107</td>\n",
       "      <td>39.955247</td>\n",
       "      <td>-75.155409</td>\n",
       "      <td>[RestaurantsGoodForGroups: True, RestaurantsTa...</td>\n",
       "      <td>noodles, seafood, restaurants, chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Outstanding venue, great band, and the food is...</td>\n",
       "      <td>2015-10-17</td>\n",
       "      <td>Amari's Restaurant</td>\n",
       "      <td>5037 Baltimore Ave</td>\n",
       "      <td>19143</td>\n",
       "      <td>39.947985</td>\n",
       "      <td>-75.224744</td>\n",
       "      <td>[HasTV: True, RestaurantsAttire: casual, DogsA...</td>\n",
       "      <td>soul food, american (new), breakfast &amp; brunch,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>We loved our experience at Elwood.\\n\\nWhen you...</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>Elwood</td>\n",
       "      <td>1007 Frankford Ave</td>\n",
       "      <td>19125</td>\n",
       "      <td>39.966404</td>\n",
       "      <td>-75.134227</td>\n",
       "      <td>[RestaurantsReservations: True, OutdoorSeating...</td>\n",
       "      <td>american (traditional), american (new), food, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  useful  funny  cool  \\\n",
       "0       4       0      0     0   \n",
       "1       5       0      0     0   \n",
       "2       4       2      0     0   \n",
       "3       5       0      0     0   \n",
       "4       5       4      0     2   \n",
       "\n",
       "                                                text       date  \\\n",
       "0  One of my favorite places to grab a quick bit ... 2014-05-28   \n",
       "1  I love this place! There's nothing like authen... 2013-05-28   \n",
       "2  Situated on Race street away from the main str... 2014-12-21   \n",
       "3  Outstanding venue, great band, and the food is... 2015-10-17   \n",
       "4  We loved our experience at Elwood.\\n\\nWhen you... 2019-05-04   \n",
       "\n",
       "                  name             address postal_code   latitude  longitude  \\\n",
       "0   Chickie's & Pete's     1526 Packer Ave       19145  39.911417 -75.174511   \n",
       "1  Guacamole Mex-Grill   4612 Woodland Ave       19143  39.943971 -75.209914   \n",
       "2       Shiao Lan Kung         930 Race St       19107  39.955247 -75.155409   \n",
       "3   Amari's Restaurant  5037 Baltimore Ave       19143  39.947985 -75.224744   \n",
       "4               Elwood  1007 Frankford Ave       19125  39.966404 -75.134227   \n",
       "\n",
       "                                          attributes  \\\n",
       "0  [GoodForKids: True, Corkage: False, HasTV: Tru...   \n",
       "1  [Caters: False, RestaurantsAttire: casual, Noi...   \n",
       "2  [RestaurantsGoodForGroups: True, RestaurantsTa...   \n",
       "3  [HasTV: True, RestaurantsAttire: casual, DogsA...   \n",
       "4  [RestaurantsReservations: True, OutdoorSeating...   \n",
       "\n",
       "                                          categories  \n",
       "0  seafood, nightlife, sports bars, bars, restaur...  \n",
       "1  mexican, restaurants, specialty food, ethnic f...  \n",
       "2             noodles, seafood, restaurants, chinese  \n",
       "3  soul food, american (new), breakfast & brunch,...  \n",
       "4  american (traditional), american (new), food, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format-wise the data look good\n",
    "restos_philly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>102836.000000</td>\n",
       "      <td>102836.000000</td>\n",
       "      <td>102836.000000</td>\n",
       "      <td>102836.000000</td>\n",
       "      <td>102836</td>\n",
       "      <td>102836.000000</td>\n",
       "      <td>102836.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.813032</td>\n",
       "      <td>1.096289</td>\n",
       "      <td>0.368460</td>\n",
       "      <td>0.544576</td>\n",
       "      <td>2015-12-06 03:08:51.409233920</td>\n",
       "      <td>39.961365</td>\n",
       "      <td>-75.160112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2005-05-26 00:00:00</td>\n",
       "      <td>39.865466</td>\n",
       "      <td>-75.363647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013-06-18 00:00:00</td>\n",
       "      <td>39.947539</td>\n",
       "      <td>-75.171477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016-03-23 00:00:00</td>\n",
       "      <td>39.950948</td>\n",
       "      <td>-75.161766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2018-08-19 00:00:00</td>\n",
       "      <td>39.961212</td>\n",
       "      <td>-75.150048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>2022-01-19 00:00:00</td>\n",
       "      <td>40.141488</td>\n",
       "      <td>-74.940729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.303007</td>\n",
       "      <td>2.476457</td>\n",
       "      <td>1.378197</td>\n",
       "      <td>1.795871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035640</td>\n",
       "      <td>0.035608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rating         useful          funny           cool  \\\n",
       "count  102836.000000  102836.000000  102836.000000  102836.000000   \n",
       "mean        3.813032       1.096289       0.368460       0.544576   \n",
       "min         1.000000       0.000000       0.000000       0.000000   \n",
       "25%         3.000000       0.000000       0.000000       0.000000   \n",
       "50%         4.000000       0.000000       0.000000       0.000000   \n",
       "75%         5.000000       1.000000       0.000000       1.000000   \n",
       "max         5.000000     115.000000      82.000000     112.000000   \n",
       "std         1.303007       2.476457       1.378197       1.795871   \n",
       "\n",
       "                                date       latitude      longitude  \n",
       "count                         102836  102836.000000  102836.000000  \n",
       "mean   2015-12-06 03:08:51.409233920      39.961365     -75.160112  \n",
       "min              2005-05-26 00:00:00      39.865466     -75.363647  \n",
       "25%              2013-06-18 00:00:00      39.947539     -75.171477  \n",
       "50%              2016-03-23 00:00:00      39.950948     -75.161766  \n",
       "75%              2018-08-19 00:00:00      39.961212     -75.150048  \n",
       "max              2022-01-19 00:00:00      40.141488     -74.940729  \n",
       "std                              NaN       0.035640       0.035608  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restos_philly.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating copy of the restaurant data\n",
    "# batch_n = int(5e4)\n",
    "# restos_philly.iloc[:batch_n, :].to_csv(\"restos_philly_batch1.csv\", index=False)\n",
    "# restos_philly.iloc[batch_n:, :].to_csv(\"restos_philly_batch2.csv\", index=False)\n",
    "\n",
    "\n",
    "# # Loading the pre-processed data in csv files\n",
    "# restos_b1 = pd.read_csv(\"restos_philly_batch1.csv\")\n",
    "# restos_b2 = pd.read_csv(\"restos_philly_batch2.csv\")\n",
    "# restos_all = pd.concat([restos_b1, restos_b2])\n",
    "# assert restos_all.shape == restos_philly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the restaurant reviews data\n",
    "\n",
    "Now that we have extracted the restaurant review data in Philadelphia, let's start by some exploratory data analysis (EDA) to make sense of the data and (hopefully) find some inspirations of what advice can be given to the business owner. To begin with, how often is each restaurant reviewed by customers in Philly? The histogram below indicates that very few restaurants have received more than 30 reviews. \n",
    "\n",
    "<!-- Some EDAs\n",
    "1. Distribution of the count of reviews per resto\n",
    "2. \n",
    "-->\n",
    "\n",
    "<!-- Some ideas of the analysis: \n",
    "1. Use embeddings on reviews and categories and then run LSA to summarise them as \"themes\" (e.g., what is a review about in terms of food, service etc.? How can we group many categories into one based on their similarity?) \n",
    "2. Analyse how the themes are related to rating to understand what customers like/ don't like about\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "variable=count<br>value=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "count",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "count",
         "offsetgroup": "count",
         "orientation": "v",
         "showlegend": true,
         "type": "histogram",
         "x": [
          888,
          624,
          560,
          530,
          523,
          511,
          505,
          440,
          439,
          426,
          417,
          414,
          395,
          389,
          384,
          351,
          343,
          324,
          323,
          305,
          303,
          299,
          281,
          272,
          268,
          264,
          243,
          242,
          237,
          234,
          225,
          224,
          222,
          221,
          217,
          214,
          214,
          214,
          213,
          211,
          209,
          205,
          199,
          199,
          198,
          194,
          194,
          193,
          188,
          187,
          184,
          183,
          182,
          181,
          181,
          178,
          176,
          174,
          174,
          172,
          170,
          168,
          167,
          165,
          163,
          163,
          162,
          161,
          160,
          159,
          159,
          158,
          158,
          157,
          157,
          156,
          155,
          155,
          155,
          149,
          149,
          149,
          147,
          147,
          144,
          143,
          143,
          143,
          142,
          142,
          140,
          138,
          137,
          137,
          136,
          135,
          133,
          133,
          133,
          133,
          133,
          132,
          132,
          131,
          130,
          130,
          130,
          129,
          129,
          128,
          128,
          128,
          127,
          126,
          125,
          124,
          124,
          123,
          122,
          122,
          121,
          120,
          120,
          120,
          119,
          119,
          119,
          119,
          119,
          118,
          118,
          117,
          117,
          116,
          116,
          115,
          115,
          115,
          114,
          114,
          114,
          114,
          114,
          114,
          114,
          113,
          112,
          111,
          110,
          110,
          110,
          109,
          109,
          109,
          108,
          108,
          108,
          108,
          107,
          107,
          106,
          106,
          106,
          105,
          105,
          105,
          105,
          105,
          104,
          104,
          104,
          103,
          103,
          103,
          102,
          102,
          101,
          101,
          101,
          101,
          100,
          100,
          100,
          99,
          99,
          98,
          98,
          98,
          98,
          97,
          96,
          96,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          95,
          94,
          93,
          93,
          93,
          92,
          92,
          91,
          91,
          91,
          91,
          91,
          91,
          90,
          90,
          90,
          90,
          90,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          89,
          88,
          88,
          88,
          87,
          87,
          86,
          86,
          85,
          85,
          85,
          85,
          85,
          85,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          84,
          83,
          83,
          83,
          83,
          83,
          83,
          82,
          82,
          82,
          82,
          81,
          81,
          81,
          81,
          81,
          80,
          80,
          80,
          79,
          79,
          79,
          79,
          79,
          78,
          78,
          78,
          78,
          77,
          77,
          77,
          77,
          76,
          76,
          76,
          76,
          76,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          75,
          74,
          74,
          74,
          74,
          74,
          74,
          74,
          73,
          73,
          73,
          73,
          73,
          73,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          72,
          71,
          71,
          71,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          70,
          69,
          69,
          69,
          69,
          69,
          69,
          69,
          68,
          68,
          68,
          68,
          68,
          67,
          67,
          67,
          67,
          67,
          67,
          66,
          66,
          66,
          66,
          66,
          66,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          65,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          64,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          63,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          62,
          61,
          61,
          61,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          60,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          59,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          58,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          57,
          56,
          56,
          56,
          56,
          56,
          56,
          56,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          55,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          54,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          53,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          52,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          51,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          50,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          49,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          48,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          47,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          46,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          45,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          44,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          43,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          42,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          41,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          40,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          39,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          38,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          37,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          36,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          34,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          33,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          32,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          31,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          29,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          28,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          27,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          26,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          25,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          24,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          23,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          22,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          21,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": " Histogram of the number of customer reviews per restaurant in Philadelphia"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Number of customer reviews"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Count of restaurants"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of reviews per restaurant\n",
    "review_counts = restos_philly[\"name\"].value_counts()\n",
    "\n",
    "fig = px.histogram(review_counts, \n",
    "                   title = \" Histogram of the number of customer reviews per restaurant in Philadelphia\")\n",
    "\n",
    "fig.update_layout(xaxis_title = \"Number of customer reviews\", \n",
    "                  yaxis_title = \"Count of restaurants\", \n",
    "                  showlegend = False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, which are the most frequently reviews restaurants in the city? And what are their average ratings and types of restaurant? We can observe from below that the cuisine styles of the top 20 most reviewed restaurants are quite diverse, as we've cafes, Chinese restaurants and steakhouses among other types. Furthermore, most of the restaurants receive average ratings of at least 4, which could imply that customers in Philadelphia tend to give their reviews to restaurants which they like, albeit there is an exception of Geno's Steaks which only has an average rating of around 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_received</th>\n",
       "      <th>average_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Reading Terminal Market</th>\n",
       "      <td>888</td>\n",
       "      <td>4.603604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pat's King of Steaks</th>\n",
       "      <td>624</td>\n",
       "      <td>3.205128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Green Eggs Café</th>\n",
       "      <td>560</td>\n",
       "      <td>3.882143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sabrina's Café</th>\n",
       "      <td>530</td>\n",
       "      <td>4.173585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dim Sum Garden</th>\n",
       "      <td>523</td>\n",
       "      <td>3.923518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>El Vez</th>\n",
       "      <td>511</td>\n",
       "      <td>4.017613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geno's Steaks</th>\n",
       "      <td>505</td>\n",
       "      <td>2.493069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barbuzzo</th>\n",
       "      <td>440</td>\n",
       "      <td>4.279545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zahav</th>\n",
       "      <td>439</td>\n",
       "      <td>4.526196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parc</th>\n",
       "      <td>426</td>\n",
       "      <td>4.239437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Han Dynasty</th>\n",
       "      <td>417</td>\n",
       "      <td>4.127098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Federal Donuts</th>\n",
       "      <td>414</td>\n",
       "      <td>4.089372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jim's South St</th>\n",
       "      <td>395</td>\n",
       "      <td>3.663291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dalessandro’s Steaks &amp; Hoagies</th>\n",
       "      <td>389</td>\n",
       "      <td>4.267352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Dandelion</th>\n",
       "      <td>384</td>\n",
       "      <td>4.263021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monk's Cafe</th>\n",
       "      <td>351</td>\n",
       "      <td>3.968661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nan Zhou Hand Drawn Noodle House</th>\n",
       "      <td>343</td>\n",
       "      <td>4.145773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Talula's Garden</th>\n",
       "      <td>324</td>\n",
       "      <td>4.453704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tommy DiNic's</th>\n",
       "      <td>323</td>\n",
       "      <td>4.043344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amada</th>\n",
       "      <td>305</td>\n",
       "      <td>4.222951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  reviews_received  average_rating\n",
       "name                                                              \n",
       "Reading Terminal Market                        888        4.603604\n",
       "Pat's King of Steaks                           624        3.205128\n",
       "Green Eggs Café                                560        3.882143\n",
       "Sabrina's Café                                 530        4.173585\n",
       "Dim Sum Garden                                 523        3.923518\n",
       "El Vez                                         511        4.017613\n",
       "Geno's Steaks                                  505        2.493069\n",
       "Barbuzzo                                       440        4.279545\n",
       "Zahav                                          439        4.526196\n",
       "Parc                                           426        4.239437\n",
       "Han Dynasty                                    417        4.127098\n",
       "Federal Donuts                                 414        4.089372\n",
       "Jim's South St                                 395        3.663291\n",
       "Dalessandro’s Steaks & Hoagies                 389        4.267352\n",
       "The Dandelion                                  384        4.263021\n",
       "Monk's Cafe                                    351        3.968661\n",
       "Nan Zhou Hand Drawn Noodle House               343        4.145773\n",
       "Talula's Garden                                324        4.453704\n",
       "Tommy DiNic's                                  323        4.043344\n",
       "Amada                                          305        4.222951"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting top 20 restaurants in terms of reviews received\n",
    "top_20_counts = review_counts[:20]\n",
    "\n",
    "# Getting average ratings of top 20 restos\n",
    "top_20_restos = top_20_counts.index\n",
    "top_20_avg_ratings = restos_philly.query(\"name in @top_20_restos\").groupby(\"name\")[\"rating\"].mean()\n",
    "\n",
    "top_20s = pd.DataFrame(data = {\"reviews_received\": top_20_counts, \n",
    "                               \"average_rating\": top_20_avg_ratings}, \n",
    "                       index = top_20_restos)\n",
    "top_20s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we also look at the proportion of reviews about the top 20 most reviewed restaurants out of all the restaurants in the dataset. They altogether received about 9% of the reviews from customers in Philadelphia, which is not a small proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4683 restaurants in Philadelphia under the current dataset, and the top 20 most reviewed restaurants received 8.84% of reviews.\n"
     ]
    }
   ],
   "source": [
    "n_restos = len(restos_philly[\"name\"].unique())\n",
    "print(f\"There are {n_restos} restaurants in Philadelphia under the current dataset, and the top 20 most reviewed restaurants received {sum(top_20s[\"reviews_received\"]) / restos_philly.shape[0] * 100:.2f}% of reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the distribution of the number of reviews among restaurants, we shall also explore how the customer ratings evolved throughout the period covered by the data. The dates of the reviews are originally recorded on the second level which will be too fine grained for understanding the rating trends. Therefore, we will start by aggregating the dates on a monthly level. the restaurant reviews covered in this dataset were created from May 2005 to January 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                           102836\n",
       "mean     2015-11-21 10:41:11.508032256\n",
       "min                2005-05-01 00:00:00\n",
       "25%                2013-06-01 00:00:00\n",
       "50%                2016-03-01 00:00:00\n",
       "75%                2018-08-01 00:00:00\n",
       "max                2022-01-01 00:00:00\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregating dates to monthly level\n",
    "restos_philly[\"date\"] = restos_philly[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "restos_philly[\"date\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (806451605.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 24\u001b[1;36m\u001b[0m\n\u001b[1;33m    fig.update_layout(title_text = \"Trends of customer reviews \"\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Seeing how the average ratings evolved as a function of time\n",
    "monthly_avg_rating = restos_philly.groupby(\"date\")[\"rating\"].apply(np.mean)\n",
    "monthly_rating_no = restos_philly.groupby(\"date\")[\"rating\"].count()\n",
    "\n",
    "monthly_trend = pd.DataFrame({\"review_no\": monthly_rating_no, \n",
    "                              \"avg_rating\": monthly_avg_rating})\n",
    "\n",
    "fig = make_subplots(rows = 1, cols = 2)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Line(x = monthly_trend.index.to_timestamp(),\n",
    "            y = monthly_trend[\"avg_rating\"]), \n",
    "    row = 1, col = 1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Line(x = monthly_trend.index.to_timestamp(),\n",
    "            y = monthly_trend[\"review_no\"]), \n",
    "    row = 1, col = 2\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text = \"Trends of customer reviews \"\n",
    "    xaxis1_title = \"Time\", yaxis1_title = \"Average rating\", \n",
    "                  xaxis2_title = \"Time\", yaxis2_title = \"Number of reviews\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting themes from reviews and restaurant categories to understand customer preferences\n",
    "\n",
    "After having a brief look at the data, it is time to tackle the major question: what do the restaurant customers in Philadelphia like or not like in general? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenji\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-trained BERT embeddings\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "bert_embed = pipeline(\"feature-extraction\", model = \"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = bert_embed(restos_philly['text'][:100].to_list(), return_tensors = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.3121,  0.0322,  0.0291,  ..., -0.0641, -0.0143,  0.2635],\n",
       "          [ 0.5117, -0.0331, -0.1938,  ..., -0.5093,  0.0722,  0.4472],\n",
       "          [ 0.4997,  0.3025, -0.7339,  ..., -0.3261, -0.2332,  0.4957],\n",
       "          ...,\n",
       "          [-0.1903, -0.0253, -0.0700,  ...,  0.2763,  0.2626,  0.2923],\n",
       "          [ 0.2809,  0.1789,  0.3817,  ...,  0.3642, -0.1232,  0.4547],\n",
       "          [ 0.5439,  0.1422, -0.5073,  ...,  0.1238, -0.0565,  0.6292]]]),\n",
       " tensor([[[ 0.3197,  0.1038,  0.0397,  ..., -0.1860,  0.0175,  0.1515],\n",
       "          [ 0.1205, -0.4394,  0.3188,  ..., -0.1101,  0.1907, -0.2168],\n",
       "          [ 0.4030, -0.0637, -0.3243,  ...,  0.2959, -0.1073,  0.1279],\n",
       "          ...,\n",
       "          [ 0.3440,  0.4152, -0.1210,  ...,  0.0634, -0.3721,  0.2837],\n",
       "          [ 0.4155,  0.1605, -0.0336,  ...,  0.0982,  0.0554,  0.1235],\n",
       "          [ 0.6464,  0.0684, -0.0862,  ..., -0.1272, -0.0960,  0.2245]]]),\n",
       " tensor([[[ 0.5042, -0.0076, -0.0589,  ..., -0.1521,  0.0667,  0.1409],\n",
       "          [ 0.3745, -0.6345,  0.3419,  ..., -0.2248,  0.2623, -0.0026],\n",
       "          [ 0.3558, -0.6065,  0.2343,  ...,  0.0874,  0.0304, -0.3960],\n",
       "          ...,\n",
       "          [ 0.0902, -0.3777, -0.5554,  ...,  0.1674,  0.6396,  0.5640],\n",
       "          [ 0.4926, -0.0493, -0.0346,  ...,  0.2098, -0.1064,  0.0203],\n",
       "          [ 1.3064,  0.1310, -0.0073,  ...,  0.4250, -0.1166,  0.2576]]]),\n",
       " tensor([[[ 0.5206, -0.0077, -0.0338,  ..., -0.1097,  0.0665,  0.2196],\n",
       "          [ 0.1121, -0.1414, -0.2023,  ..., -0.4122,  0.2029,  0.3956],\n",
       "          [ 0.0558, -0.0012, -0.1926,  ..., -0.1402, -0.2335,  0.0886],\n",
       "          ...,\n",
       "          [ 0.5466,  0.0895,  0.0610,  ...,  0.2587, -0.1501,  0.1756],\n",
       "          [ 0.5437, -0.0209,  0.2240,  ...,  0.2315, -0.0642,  0.2238],\n",
       "          [ 0.8805,  0.6389, -0.0236,  ...,  0.0242,  0.3820,  0.7593]]]),\n",
       " tensor([[[ 0.3873,  0.0735,  0.1285,  ..., -0.1139,  0.0817,  0.1447],\n",
       "          [ 0.4734, -0.7624, -0.0576,  ...,  0.0794,  0.2828, -0.1308],\n",
       "          [ 0.3322, -0.2343, -0.2614,  ...,  0.3799, -0.1692,  0.2379],\n",
       "          ...,\n",
       "          [ 0.1272, -0.2099,  0.1548,  ..., -0.2773,  0.0356, -0.0709],\n",
       "          [ 0.3853,  0.0160,  0.3630,  ...,  0.2037,  0.2611,  0.1395],\n",
       "          [ 0.9016,  0.9348,  0.0443,  ...,  0.0801,  0.3187,  0.2980]]]),\n",
       " tensor([[[ 2.4169e-01, -1.5211e-02,  1.6414e-01,  ..., -1.4016e-01,\n",
       "            1.2072e-01,  2.4649e-01],\n",
       "          [ 3.2144e-01, -3.2380e-01,  8.0707e-02,  ...,  1.9306e-01,\n",
       "            3.0955e-01,  2.9320e-01],\n",
       "          [ 3.4634e-01, -2.3989e-01, -4.5390e-02,  ..., -4.7962e-03,\n",
       "            2.2988e-01,  4.1448e-01],\n",
       "          ...,\n",
       "          [ 2.3744e-01,  3.1551e-01,  4.9028e-01,  ...,  6.8382e-02,\n",
       "           -6.8953e-04,  5.4567e-01],\n",
       "          [ 2.8592e-01, -1.0388e-01,  3.3314e-01,  ...,  2.0145e-02,\n",
       "            4.9804e-02,  2.7613e-01],\n",
       "          [ 4.6736e-01,  1.6126e-01, -1.3493e-01,  ..., -2.1122e-01,\n",
       "           -8.9300e-02,  7.3998e-01]]]),\n",
       " tensor([[[ 0.2833,  0.1023,  0.0074,  ..., -0.0876,  0.0012,  0.1877],\n",
       "          [ 0.3586,  0.0082,  0.0138,  ..., -0.3732,  0.4352,  0.1263],\n",
       "          [ 0.3958, -0.0621,  0.0800,  ..., -0.0832,  0.2695,  0.2680],\n",
       "          ...,\n",
       "          [ 0.3796,  0.2990, -0.0570,  ..., -0.0970, -0.9596,  0.3974],\n",
       "          [ 0.3127,  0.1881,  0.1594,  ...,  0.1088, -0.1871,  0.1285],\n",
       "          [ 0.3455,  0.1818, -0.3694,  ...,  0.3036, -0.0941,  0.5293]]]),\n",
       " tensor([[[ 0.3712,  0.0317, -0.0012,  ..., -0.1017,  0.0147,  0.2601],\n",
       "          [ 0.0312, -0.1338,  0.1798,  ..., -0.1245,  0.3059,  0.2078],\n",
       "          [ 0.4856,  0.3793, -0.0331,  ..., -0.1416, -0.2050,  0.0714],\n",
       "          ...,\n",
       "          [ 0.0306, -0.2902,  0.1688,  ...,  0.2792, -0.0899,  0.4721],\n",
       "          [ 0.2587, -0.0108,  0.3185,  ...,  0.1921, -0.1355,  0.2678],\n",
       "          [ 0.9693,  0.2042, -0.5144,  ...,  0.2861, -0.2613,  0.6671]]]),\n",
       " tensor([[[ 0.3761,  0.0083, -0.0017,  ..., -0.1413,  0.0908,  0.2512],\n",
       "          [ 0.4661, -0.3894,  0.4044,  ..., -0.4833, -0.2358,  0.1025],\n",
       "          [ 0.2016,  0.0525, -0.1248,  ..., -0.1877,  0.1717,  0.1202],\n",
       "          ...,\n",
       "          [ 0.1928,  0.0114, -0.4351,  ...,  0.0177,  0.2554,  0.1915],\n",
       "          [ 0.2181,  0.3052,  0.2749,  ..., -0.0527, -0.2746,  0.5100],\n",
       "          [ 0.8706,  0.1061, -0.0301,  ...,  0.2471, -0.3419,  0.4310]]]),\n",
       " tensor([[[ 0.3071,  0.1296, -0.0067,  ..., -0.1172,  0.0167,  0.0875],\n",
       "          [ 0.1170, -0.4066,  0.4707,  ..., -0.1713,  0.5116,  0.0124],\n",
       "          [-0.3572, -0.2160,  0.4894,  ..., -0.2926,  0.0694,  0.2195],\n",
       "          ...,\n",
       "          [ 0.4939, -0.2126,  0.1024,  ...,  0.0386,  0.1468,  0.1260],\n",
       "          [ 0.2525,  0.2294,  0.2627,  ...,  0.3696, -0.0188, -0.0191],\n",
       "          [ 0.9969,  0.4416, -0.2129,  ...,  0.2637, -0.2899,  0.2386]]]),\n",
       " tensor([[[ 0.3780,  0.1155, -0.0661,  ..., -0.1693,  0.0152,  0.1355],\n",
       "          [ 0.1309,  0.2352,  0.1605,  ...,  0.2079, -0.5242,  0.4693],\n",
       "          [-0.0546, -0.0572,  0.0944,  ...,  0.1602,  0.0669,  0.1148],\n",
       "          ...,\n",
       "          [-0.2054, -0.1422,  0.0629,  ...,  0.0458, -0.0518,  0.7252],\n",
       "          [ 0.0795, -0.0930,  0.1864,  ...,  0.0293, -0.1749,  0.0460],\n",
       "          [ 0.7766,  0.3230, -0.3492,  ...,  0.1951, -0.0359,  0.4194]]]),\n",
       " tensor([[[ 2.6822e-01,  6.4342e-02, -1.8060e-04,  ..., -1.4721e-01,\n",
       "            6.5676e-02,  1.0404e-01],\n",
       "          [ 1.1883e-01,  4.2305e-01, -2.4633e-02,  ..., -2.1563e-01,\n",
       "            1.9197e-01,  1.0897e-01],\n",
       "          [ 2.1625e-02, -4.0396e-02,  1.8980e-01,  ...,  4.5354e-02,\n",
       "            2.3220e-01, -4.0404e-02],\n",
       "          ...,\n",
       "          [ 2.9546e-01, -2.1014e-01,  8.9948e-02,  ...,  3.9213e-02,\n",
       "            1.8062e-01, -2.7124e-02],\n",
       "          [ 3.5207e-01, -1.3298e-01,  1.4949e-01,  ...,  9.4873e-02,\n",
       "           -4.2258e-02,  1.8438e-02],\n",
       "          [ 6.3690e-01,  1.9565e-01, -5.2176e-01,  ..., -2.3772e-02,\n",
       "            1.4574e-02,  2.9607e-01]]]),\n",
       " tensor([[[ 0.2909,  0.2409, -0.0356,  ..., -0.0426,  0.1425,  0.0635],\n",
       "          [-0.2022, -0.4564,  0.3355,  ..., -0.3844,  0.2842,  0.2127],\n",
       "          [ 0.1219,  0.1419, -0.1551,  ...,  0.3026,  0.2402,  0.2432],\n",
       "          ...,\n",
       "          [-0.0536,  0.2754,  0.0100,  ..., -0.2369,  0.5129,  0.4429],\n",
       "          [ 0.0590,  0.1403,  0.0016,  ..., -0.0457,  0.2503,  0.2966],\n",
       "          [ 0.4891,  0.9134, -0.1968,  ...,  0.5584,  0.2253,  0.3721]]]),\n",
       " tensor([[[ 0.3298,  0.0051,  0.0526,  ..., -0.1357,  0.0757,  0.0983],\n",
       "          [ 0.2309, -0.0121,  0.0722,  ..., -0.3552, -0.3371,  0.0334],\n",
       "          [ 0.2304,  0.2081,  0.0312,  ...,  0.1499, -0.2836,  0.1600],\n",
       "          ...,\n",
       "          [ 0.2387,  0.5881,  0.2155,  ...,  0.3502, -0.1303,  0.4300],\n",
       "          [ 0.4743,  0.0897,  0.1999,  ...,  0.1546,  0.0068, -0.0438],\n",
       "          [ 0.6632,  0.1008, -0.1562,  ..., -0.4078,  0.3150,  0.0716]]]),\n",
       " tensor([[[ 0.2885,  0.0654,  0.0020,  ..., -0.0544,  0.0048,  0.2122],\n",
       "          [ 0.2556, -0.4544,  0.2035,  ..., -0.2794,  0.3380,  0.0591],\n",
       "          [ 0.4338,  0.0597,  0.1892,  ...,  0.0518,  0.0464, -0.0824],\n",
       "          ...,\n",
       "          [-0.1965, -0.0578, -0.0986,  ...,  0.1423,  0.2020,  0.1837],\n",
       "          [ 0.5307,  0.1459, -0.0099,  ...,  0.1342, -0.1822,  0.2147],\n",
       "          [ 0.7080,  0.1701, -0.4814,  ...,  0.1697, -0.1881,  0.3968]]]),\n",
       " tensor([[[ 0.3383,  0.1274, -0.0308,  ...,  0.0835,  0.0297,  0.0243],\n",
       "          [ 0.5592,  0.0661, -0.0276,  ...,  0.2314,  0.3007,  0.4096],\n",
       "          [ 0.2086,  0.0992,  0.1259,  ...,  0.1039,  0.3540,  0.4196],\n",
       "          ...,\n",
       "          [-0.0388,  0.6234,  0.3192,  ...,  0.4563,  0.0883,  0.1807],\n",
       "          [ 0.3143,  0.3037,  0.0582,  ...,  0.4733,  0.1285,  0.1759],\n",
       "          [ 0.4501,  0.5711, -0.1966,  ...,  0.9972,  0.2933,  0.1694]]]),\n",
       " tensor([[[ 0.1876,  0.0339,  0.0182,  ..., -0.0674,  0.1431,  0.1858],\n",
       "          [ 0.5713, -0.6511,  0.1859,  ..., -0.2099,  0.8951,  0.0273],\n",
       "          [ 0.0070, -0.3918,  0.2077,  ...,  0.0578,  0.6025, -0.3008],\n",
       "          ...,\n",
       "          [ 0.0632, -0.2009, -0.0615,  ...,  0.0227, -0.1655,  0.5085],\n",
       "          [ 0.1520,  0.0575,  0.1229,  ...,  0.1239,  0.2058,  0.1454],\n",
       "          [ 0.5444,  0.0974, -0.2572,  ..., -0.0146,  0.3500,  0.1472]]]),\n",
       " tensor([[[ 0.3204,  0.0792, -0.0294,  ..., -0.1044,  0.0617,  0.1944],\n",
       "          [ 0.1565, -0.2239,  0.3281,  ..., -0.1138,  0.1097,  0.0135],\n",
       "          [ 0.1975,  0.4757,  0.3871,  ...,  0.3374, -0.1028,  0.0358],\n",
       "          ...,\n",
       "          [-0.1630,  0.0573,  0.4126,  ...,  0.1276, -0.1644, -0.1665],\n",
       "          [ 0.7595,  0.6164, -0.4062,  ...,  0.3333, -0.0983,  0.5514],\n",
       "          [ 0.7466,  0.5463, -0.4162,  ...,  0.2347, -0.2083,  0.7240]]]),\n",
       " tensor([[[ 0.2330,  0.0295,  0.0453,  ..., -0.1123, -0.0790,  0.1585],\n",
       "          [ 0.0253, -0.4119,  0.1550,  ..., -0.2796, -0.0043,  0.0364],\n",
       "          [ 0.2634, -0.0622, -0.3905,  ..., -0.1131, -0.1185, -0.1490],\n",
       "          ...,\n",
       "          [-0.1664,  0.2753,  0.2871,  ...,  0.1339, -0.4757,  0.3674],\n",
       "          [ 0.2173, -0.1895, -0.0251,  ..., -0.0806, -0.0902,  0.0236],\n",
       "          [ 0.5883, -0.0902, -0.1693,  ...,  0.0450, -0.6430,  0.1862]]]),\n",
       " tensor([[[ 0.3731,  0.0666, -0.0437,  ..., -0.0632,  0.1703,  0.2682],\n",
       "          [ 0.0350, -0.0358,  0.2209,  ..., -0.0536,  0.2400,  0.0692],\n",
       "          [ 0.3736,  0.3984,  0.1702,  ...,  0.1529, -0.0777,  0.0358],\n",
       "          ...,\n",
       "          [ 0.4714,  0.1191, -0.1220,  ...,  0.1339,  0.2738,  0.0491],\n",
       "          [ 0.4545, -0.0906,  0.1630,  ...,  0.0828,  0.1795,  0.1908],\n",
       "          [ 0.4461,  0.0991, -0.3858,  ...,  0.0052,  0.2139,  0.4989]]]),\n",
       " tensor([[[ 0.4673,  0.0118,  0.0022,  ..., -0.1103,  0.0432,  0.1716],\n",
       "          [ 0.4812, -0.6781,  0.4046,  ..., -0.2374,  0.1989,  0.2027],\n",
       "          [ 0.3233, -0.1096,  0.0395,  ...,  0.4587,  0.4636, -0.0291],\n",
       "          ...,\n",
       "          [ 0.1331, -0.0200,  0.0529,  ...,  0.2915,  0.0133,  0.5257],\n",
       "          [ 0.4310, -0.0212,  0.1173,  ...,  0.4078, -0.1815,  0.2399],\n",
       "          [ 0.9665, -0.1487, -0.2830,  ...,  0.0661, -0.4526,  0.3888]]]),\n",
       " tensor([[[ 0.2944,  0.0343, -0.0192,  ..., -0.0733, -0.0108,  0.2065],\n",
       "          [ 0.0907, -0.2990,  0.2241,  ...,  0.0904,  0.3240,  0.2174],\n",
       "          [-0.0227, -0.0215,  0.3557,  ...,  0.4664,  0.1657,  0.3542],\n",
       "          ...,\n",
       "          [-0.2246,  0.0900, -0.5309,  ...,  0.3324,  0.3664, -0.1356],\n",
       "          [ 0.3348,  0.0250, -0.0849,  ...,  0.0577,  0.0031,  0.0733],\n",
       "          [ 0.7598,  0.2080, -0.1156,  ...,  0.4116, -0.3800,  0.4475]]]),\n",
       " tensor([[[ 0.3943,  0.1057,  0.0021,  ..., -0.1909,  0.0976,  0.0809],\n",
       "          [ 0.3137, -0.2759,  0.2160,  ..., -0.0731,  0.1468, -0.0279],\n",
       "          [ 0.4208,  0.1161, -0.2946,  ...,  0.0700, -0.0853, -0.0094],\n",
       "          ...,\n",
       "          [ 0.0310,  0.3201,  0.2852,  ...,  0.9588, -0.1672,  0.3052],\n",
       "          [ 0.4143,  0.1374,  0.1903,  ...,  0.3091, -0.0129,  0.0509],\n",
       "          [ 0.7341,  0.0898, -0.2997,  ...,  0.1021,  0.3751,  0.0709]]]),\n",
       " tensor([[[ 0.2048,  0.0521, -0.0364,  ..., -0.1922, -0.0688,  0.0754],\n",
       "          [ 0.0186, -0.3129, -0.0934,  ..., -0.2119, -0.1643, -0.0511],\n",
       "          [ 0.0324, -0.0675,  0.0878,  ..., -0.1131, -0.5276,  0.1124],\n",
       "          ...,\n",
       "          [-0.3839,  0.2735,  0.3370,  ...,  0.3622, -0.6252,  0.0864],\n",
       "          [ 0.0944, -0.0579,  0.2411,  ...,  0.0544, -0.2406,  0.0348],\n",
       "          [ 0.5383, -0.2379,  0.1466,  ..., -0.1697, -0.9134,  0.1536]]]),\n",
       " tensor([[[ 0.3133,  0.1843, -0.0194,  ..., -0.0254,  0.0127,  0.0684],\n",
       "          [ 0.0064, -0.1642, -0.1962,  ...,  0.0157, -0.0449,  0.0929],\n",
       "          [ 0.3317,  0.3713,  0.2226,  ..., -0.0235, -0.0720, -0.1795],\n",
       "          ...,\n",
       "          [ 0.1186,  0.3883,  0.5070,  ...,  0.4310,  0.0412, -0.0320],\n",
       "          [ 0.1928,  0.2764,  0.2081,  ...,  0.3892, -0.1977,  0.3655],\n",
       "          [ 0.2049,  0.4455, -0.3327,  ...,  0.3582, -0.1170,  0.4523]]]),\n",
       " tensor([[[ 0.3027, -0.0365, -0.0849,  ..., -0.0090,  0.2009,  0.1375],\n",
       "          [ 0.0145,  0.0313,  0.1065,  ...,  0.1642, -0.2974,  0.5625],\n",
       "          [-0.1908, -0.1009, -0.0404,  ...,  0.2886,  0.2862,  0.1087],\n",
       "          ...,\n",
       "          [ 0.3513,  0.0699, -0.3474,  ...,  0.4635,  0.8103,  0.2172],\n",
       "          [ 0.6012,  0.1120, -0.3899,  ...,  0.5287, -0.0943, -0.0114],\n",
       "          [ 0.9154,  0.8543, -0.3290,  ...,  0.0302,  0.8676,  0.3471]]]),\n",
       " tensor([[[ 0.3809,  0.0292, -0.1295,  ..., -0.1991,  0.0293,  0.1305],\n",
       "          [ 0.3647, -0.7467,  0.4017,  ..., -0.3889,  0.1918,  0.1014],\n",
       "          [-0.1035, -0.0706, -0.2609,  ..., -0.3316, -0.1559,  0.1979],\n",
       "          ...,\n",
       "          [ 0.3973, -0.3956, -0.1411,  ...,  0.4664,  0.0259,  0.0520],\n",
       "          [ 0.4185,  0.0314,  0.1489,  ...,  0.4207, -0.0413,  0.0699],\n",
       "          [ 0.7632,  0.1737, -0.3092,  ...,  0.6310, -0.3960, -0.0601]]]),\n",
       " tensor([[[ 0.2777,  0.0767,  0.0118,  ..., -0.0354,  0.0766,  0.0935],\n",
       "          [ 0.3363, -0.1474,  0.5150,  ..., -0.0747,  0.1837, -0.1070],\n",
       "          [ 0.2977,  0.0400, -0.3257,  ...,  0.1306, -0.3410, -0.0290],\n",
       "          ...,\n",
       "          [-0.2154,  1.0372,  0.3077,  ...,  0.5573,  0.4069,  0.0581],\n",
       "          [ 0.1749,  0.3714,  0.2296,  ...,  0.3058,  0.2904, -0.0149],\n",
       "          [ 0.4012,  0.2060, -0.3432,  ...,  0.8748,  0.2023,  0.0855]]]),\n",
       " tensor([[[ 0.2942,  0.0872, -0.0055,  ..., -0.1213,  0.1021, -0.0156],\n",
       "          [-0.2753,  0.1249,  0.1176,  ..., -0.2785,  0.0655,  0.2729],\n",
       "          [ 0.2246,  0.1044, -0.0210,  ..., -0.2801,  0.4207, -0.0805],\n",
       "          ...,\n",
       "          [-0.1571,  0.1889,  0.2999,  ..., -0.0032,  0.1426,  0.2711],\n",
       "          [ 0.1493, -0.0523,  0.2561,  ..., -0.0344,  0.1723, -0.2091],\n",
       "          [ 0.6348,  0.4726, -0.3020,  ..., -0.0722,  0.5707,  0.0231]]]),\n",
       " tensor([[[ 0.3086,  0.0463, -0.0496,  ..., -0.1493,  0.0593,  0.1845],\n",
       "          [ 0.1458, -0.3930,  0.2020,  ..., -0.2741,  0.3559,  0.1535],\n",
       "          [ 0.4539,  0.0802, -0.0701,  ..., -0.0811, -0.1859, -0.0052],\n",
       "          ...,\n",
       "          [-0.4014,  0.4285, -0.5965,  ..., -0.0653, -0.5989,  0.4040],\n",
       "          [ 0.3380,  0.0313,  0.1101,  ...,  0.1044, -0.1159,  0.2974],\n",
       "          [ 0.5155,  0.1318, -0.3221,  ...,  0.2294, -0.2881,  0.1857]]]),\n",
       " tensor([[[ 0.3043,  0.1143, -0.0553,  ..., -0.1941,  0.0274,  0.1350],\n",
       "          [ 0.3573, -0.5169,  0.2684,  ..., -0.2247,  0.2100,  0.1108],\n",
       "          [ 0.0582,  0.0649, -0.3076,  ...,  0.4317,  0.5617, -0.1231],\n",
       "          ...,\n",
       "          [ 0.2164, -0.1266,  0.0915,  ...,  0.1390, -0.0991,  0.0410],\n",
       "          [ 0.1725,  0.1422,  0.0594,  ...,  0.1311, -0.0998, -0.0164],\n",
       "          [ 0.8107,  0.3089, -0.4015,  ...,  0.4578, -0.6485,  0.2516]]]),\n",
       " tensor([[[ 0.3361,  0.1992, -0.1236,  ..., -0.0274, -0.0421,  0.0544],\n",
       "          [ 0.1845, -0.2724, -0.0009,  ..., -0.2687,  0.4032,  0.0227],\n",
       "          [ 0.5523,  0.0632, -0.2436,  ..., -0.0567, -0.1724, -0.2390],\n",
       "          ...,\n",
       "          [ 0.2790,  0.2541, -0.0512,  ...,  0.2690,  0.5075, -0.3443],\n",
       "          [ 0.4708,  0.3865,  0.2522,  ...,  0.3441, -0.1440,  0.0052],\n",
       "          [ 0.7796,  0.1891, -0.6946,  ...,  0.5698, -0.0608,  0.1718]]]),\n",
       " tensor([[[ 0.3259,  0.0543, -0.0274,  ..., -0.1340,  0.0511,  0.1402],\n",
       "          [ 0.2537, -0.7609,  0.2544,  ..., -0.2588,  0.3083, -0.0162],\n",
       "          [ 0.1673, -0.0102, -0.3083,  ...,  0.4901,  0.4560, -0.0645],\n",
       "          ...,\n",
       "          [ 0.0113,  0.2910,  0.1908,  ...,  0.3292,  0.0398, -0.2595],\n",
       "          [ 0.4733,  0.1134, -0.0255,  ...,  0.0777, -0.0023,  0.2029],\n",
       "          [ 0.9052,  0.3537, -0.4103,  ...,  0.3804, -0.3511,  0.1966]]]),\n",
       " tensor([[[ 0.3382,  0.0374, -0.1329,  ..., -0.1385, -0.0009,  0.1093],\n",
       "          [ 0.2792, -0.4031,  0.0618,  ..., -0.1524,  0.0695,  0.0510],\n",
       "          [ 0.0760, -0.1471, -0.0093,  ...,  0.0512, -0.2167,  0.1971],\n",
       "          ...,\n",
       "          [ 0.2504, -0.0810, -0.4268,  ...,  0.3619,  0.2228,  0.3470],\n",
       "          [ 0.4679, -0.0664, -0.0367,  ...,  0.2175, -0.1176, -0.1158],\n",
       "          [ 0.7264,  0.0456, -0.2286,  ...,  0.6900, -0.7220,  0.1206]]]),\n",
       " tensor([[[ 0.3581,  0.0343, -0.0307,  ..., -0.0318,  0.2209,  0.2352],\n",
       "          [-0.0258, -0.0763,  0.0364,  ..., -0.3077,  0.1062,  0.0026],\n",
       "          [-0.1564, -0.0684,  0.0912,  ...,  0.1595, -0.0340,  0.0666],\n",
       "          ...,\n",
       "          [ 0.4444,  0.1099,  0.0145,  ...,  0.4444, -0.5009,  0.4278],\n",
       "          [-0.0371, -0.0337, -0.2653,  ..., -0.1018, -0.0697,  0.4792],\n",
       "          [ 0.5174,  0.1725, -0.7207,  ...,  0.2445,  0.4736,  0.4204]]]),\n",
       " tensor([[[ 3.3866e-01, -2.3293e-02,  8.7543e-02,  ...,  2.7003e-03,\n",
       "            4.6318e-02,  1.6484e-01],\n",
       "          [ 3.4985e-01,  8.7199e-03,  3.5528e-01,  ..., -1.6426e-01,\n",
       "            2.3674e-01, -4.5335e-02],\n",
       "          [ 6.6993e-01,  2.8490e-01,  5.5197e-01,  ...,  6.5198e-02,\n",
       "            4.7319e-02,  3.3770e-01],\n",
       "          ...,\n",
       "          [ 4.7058e-01, -4.5080e-01, -3.1192e-01,  ...,  5.5569e-01,\n",
       "            2.5451e-01,  2.3551e-01],\n",
       "          [ 4.4360e-01, -9.6312e-02, -2.7723e-04,  ...,  2.2318e-01,\n",
       "           -2.3360e-03,  2.0654e-01],\n",
       "          [ 8.7963e-01,  4.1599e-01, -3.4799e-02,  ...,  5.2199e-01,\n",
       "            1.1979e-01,  1.7737e-01]]]),\n",
       " tensor([[[ 0.3537,  0.1952, -0.1405,  ..., -0.0056,  0.0369,  0.1574],\n",
       "          [ 0.2973,  0.1705, -0.1038,  ..., -0.0475,  0.1145,  0.1809],\n",
       "          [ 0.2985,  0.1960, -0.4765,  ...,  0.3326, -0.0576,  0.1477],\n",
       "          ...,\n",
       "          [ 0.0230,  0.0504, -0.2301,  ...,  0.1335, -0.1961,  0.1818],\n",
       "          [ 0.2771,  0.4890,  0.1636,  ...,  0.3636,  0.0262,  0.1160],\n",
       "          [ 1.3222,  0.2982, -0.3000,  ...,  0.5012,  0.2392,  0.5112]]]),\n",
       " tensor([[[ 0.4249, -0.0871,  0.0290,  ..., -0.1270,  0.2089,  0.1934],\n",
       "          [ 0.2377, -0.1197,  0.2387,  ..., -0.0030,  0.2855, -0.0363],\n",
       "          [ 0.0897,  0.1717,  0.1660,  ...,  0.4929,  0.2715, -0.1663],\n",
       "          ...,\n",
       "          [ 0.0518, -0.0524, -0.1994,  ...,  0.0545,  0.6695,  0.7077],\n",
       "          [ 0.2162, -0.3984,  0.1800,  ...,  0.3761,  0.5991,  0.2468],\n",
       "          [ 0.4735,  0.3429, -0.0868,  ...,  0.1258,  0.2360,  0.3585]]]),\n",
       " tensor([[[ 0.3599,  0.0146,  0.0076,  ..., -0.1153,  0.0136,  0.2096],\n",
       "          [ 0.5317, -0.6421, -0.1785,  ...,  0.1097,  0.4970,  0.1065],\n",
       "          [ 0.3920, -0.2558, -0.2103,  ...,  0.5292,  0.3768, -0.1667],\n",
       "          ...,\n",
       "          [ 0.5197, -0.1001,  0.0098,  ..., -0.1432, -0.1992, -0.0295],\n",
       "          [ 0.3850, -0.1456,  0.1319,  ..., -0.0606, -0.0823,  0.1206],\n",
       "          [ 0.6945,  0.5835, -0.3687,  ...,  0.4362, -0.0690,  0.2890]]]),\n",
       " tensor([[[ 0.3291,  0.0584, -0.0314,  ..., -0.1913,  0.0200,  0.1888],\n",
       "          [ 0.1446, -0.1016, -0.0972,  ..., -0.2998,  0.2774,  0.0335],\n",
       "          [ 0.3233,  0.0049, -0.0598,  ...,  0.0435,  0.0057,  0.1410],\n",
       "          ...,\n",
       "          [-0.3718, -0.3197, -0.2948,  ...,  0.1126, -0.5667,  0.0523],\n",
       "          [ 0.2051,  0.0077,  0.0698,  ..., -0.1063,  0.1836, -0.0781],\n",
       "          [ 0.7161,  0.0182, -0.3037,  ..., -0.0174, -0.3487,  0.1104]]]),\n",
       " tensor([[[ 0.2992,  0.1251, -0.0659,  ..., -0.1601,  0.0320,  0.0988],\n",
       "          [ 0.2193,  0.0595, -0.2315,  ..., -0.1681,  0.0251,  0.0147],\n",
       "          [ 0.0430,  0.4315, -0.1978,  ...,  0.0137,  0.2588,  0.0481],\n",
       "          ...,\n",
       "          [ 0.3362,  0.4246, -0.2071,  ...,  0.1141, -0.0407,  0.2968],\n",
       "          [ 0.2831,  0.0577, -0.0157,  ...,  0.1375,  0.0416, -0.0836],\n",
       "          [ 0.8357,  0.2988, -0.3938,  ...,  0.0716,  0.0273,  0.2209]]]),\n",
       " tensor([[[ 0.3190,  0.0972,  0.0900,  ..., -0.1081,  0.0204,  0.1147],\n",
       "          [ 0.3053, -0.4690,  0.1210,  ..., -0.4006,  0.4192,  0.0048],\n",
       "          [ 0.3786, -0.4115,  0.4719,  ...,  0.1218,  0.1874, -0.4458],\n",
       "          ...,\n",
       "          [ 0.0201,  0.4104,  0.6414,  ...,  0.3443,  0.0503, -0.1129],\n",
       "          [ 0.3443,  0.0230,  0.2865,  ...,  0.1175, -0.1761,  0.0871],\n",
       "          [ 0.8034,  0.3710, -0.1175,  ...,  0.2622, -0.1031,  0.4350]]]),\n",
       " tensor([[[ 0.3186, -0.0503, -0.1237,  ..., -0.0771, -0.0440,  0.1878],\n",
       "          [ 0.1090, -0.3288, -0.1756,  ..., -0.2358,  0.1955,  0.2350],\n",
       "          [ 0.0914,  0.0172,  0.2881,  ..., -0.0724,  0.1684,  0.0612],\n",
       "          ...,\n",
       "          [ 0.0698, -0.1012, -0.0040,  ...,  0.5005, -0.6140,  0.0860],\n",
       "          [ 0.4537,  0.0326, -0.2362,  ...,  0.3381, -0.0798, -0.0263],\n",
       "          [ 0.4050, -0.1355, -0.3551,  ...,  0.7729, -0.7326,  0.3565]]]),\n",
       " tensor([[[ 0.2525,  0.0180,  0.0375,  ..., -0.0528,  0.0824,  0.1952],\n",
       "          [-0.1190,  0.1186,  0.0493,  ...,  0.1911,  0.1906,  0.1892],\n",
       "          [-0.0920,  0.2156,  0.3062,  ..., -0.0799,  0.2181,  0.3156],\n",
       "          ...,\n",
       "          [-0.0554, -0.2596,  0.1945,  ...,  0.0679, -0.0262,  0.2538],\n",
       "          [ 0.1678, -0.0749,  0.1884,  ...,  0.1279, -0.0846,  0.0677],\n",
       "          [ 0.1463, -0.1231, -0.3324,  ..., -0.0729,  0.7171,  0.1566]]]),\n",
       " tensor([[[ 0.2527,  0.0813,  0.0906,  ..., -0.1557,  0.1148,  0.1848],\n",
       "          [ 0.0591, -0.3209,  0.2457,  ..., -0.2117,  0.4169,  0.1844],\n",
       "          [ 0.2737, -0.0269,  0.0675,  ..., -0.1429, -0.1450, -0.0733],\n",
       "          ...,\n",
       "          [ 0.0910,  0.5614,  0.0192,  ..., -0.2490,  0.2538,  0.6366],\n",
       "          [ 0.2016,  0.3230,  0.3350,  ...,  0.0079,  0.0282,  0.1236],\n",
       "          [ 0.1689,  0.0182, -0.0571,  ..., -0.0907,  0.6903,  0.1838]]]),\n",
       " tensor([[[ 0.2983,  0.0314,  0.0211,  ..., -0.1373,  0.0725,  0.2362],\n",
       "          [ 0.2049, -0.6388,  0.1715,  ...,  0.0211,  0.2106,  0.0552],\n",
       "          [ 0.4050, -0.0687, -0.2058,  ...,  0.4272, -0.0537,  0.3135],\n",
       "          ...,\n",
       "          [-0.3274,  0.0380, -0.1816,  ..., -0.2821, -0.3941,  0.3334],\n",
       "          [ 0.4125,  0.5373, -0.1650,  ..., -0.1307,  0.1407,  0.4967],\n",
       "          [ 0.4449,  0.5588, -0.2540,  ..., -0.1305,  0.1113,  0.6668]]]),\n",
       " tensor([[[ 0.2521,  0.1475,  0.0782,  ..., -0.1538,  0.0434,  0.1154],\n",
       "          [ 0.6803,  0.1161,  0.3326,  ..., -0.0783, -0.1129,  0.0739],\n",
       "          [ 0.5007,  0.0618,  0.4868,  ..., -0.4614, -0.0202, -0.0720],\n",
       "          ...,\n",
       "          [ 0.3284,  0.1965,  0.2925,  ...,  0.4157, -0.0192,  0.1821],\n",
       "          [ 0.2796,  0.0158,  0.1556,  ..., -0.0466,  0.0600, -0.0111],\n",
       "          [ 0.8793,  0.1491, -0.2253,  ..., -0.1693,  0.2076,  0.5324]]]),\n",
       " tensor([[[ 0.1988,  0.1677,  0.0825,  ..., -0.0077, -0.0782, -0.0046],\n",
       "          [ 0.2772,  0.3234,  0.3820,  ..., -0.1879, -0.1737, -0.1488],\n",
       "          [-0.0310, -0.2629,  0.0823,  ...,  0.0830, -0.1140,  0.1269],\n",
       "          ...,\n",
       "          [-0.0124,  0.5293,  0.3394,  ...,  0.3016,  0.1697,  0.1206],\n",
       "          [ 0.2890,  0.3498,  0.2617,  ...,  0.2456,  0.1129,  0.0835],\n",
       "          [ 0.5128,  0.6236,  0.0137,  ...,  0.2853,  0.1633,  0.5684]]]),\n",
       " tensor([[[ 0.3232,  0.1303, -0.1598,  ..., -0.1144, -0.0252,  0.1384],\n",
       "          [ 0.0482, -0.6089,  0.4005,  ..., -0.6253,  0.3958, -0.1407],\n",
       "          [ 0.2114, -0.6323, -0.7359,  ..., -0.2506,  0.7528,  0.2532],\n",
       "          ...,\n",
       "          [ 0.0560,  0.2447, -0.0044,  ..., -0.1513,  0.0792,  0.5364],\n",
       "          [ 0.3582,  0.1978,  0.0626,  ...,  0.4199, -0.3151,  0.0959],\n",
       "          [ 0.9638,  0.2449, -0.4757,  ...,  0.7825, -0.6090,  0.3192]]]),\n",
       " tensor([[[ 0.4602,  0.0241, -0.1186,  ..., -0.1892,  0.1670,  0.0984],\n",
       "          [ 0.0219,  0.0423,  0.0475,  ..., -0.1606,  0.3582, -0.2492],\n",
       "          [ 0.3914,  0.3412,  0.2129,  ...,  0.2403,  0.0532, -0.1430],\n",
       "          ...,\n",
       "          [ 0.2489,  0.2421, -0.2709,  ..., -0.0450,  0.0167,  0.2399],\n",
       "          [ 0.5578, -0.1399, -0.1528,  ..., -0.0952,  0.1402,  0.0916],\n",
       "          [ 0.8402,  0.4866, -0.2247,  ...,  0.0919,  0.0407,  0.1630]]]),\n",
       " tensor([[[ 0.2814,  0.0467, -0.0055,  ..., -0.1366,  0.1572,  0.1709],\n",
       "          [ 0.2535, -0.0233,  0.0640,  ..., -0.5354,  0.3325,  0.3557],\n",
       "          [ 0.1752,  0.0378,  0.0158,  ...,  0.0355,  0.4940, -0.0042],\n",
       "          ...,\n",
       "          [-0.0205,  0.4056, -0.2145,  ..., -0.3343,  0.0555,  0.2042],\n",
       "          [ 0.0956,  0.1409,  0.1225,  ...,  0.1351,  0.1088,  0.1145],\n",
       "          [ 0.3770,  0.2971, -0.2917,  ...,  0.0921,  0.3006,  0.2365]]]),\n",
       " tensor([[[ 0.1901,  0.1225,  0.0857,  ...,  0.0814, -0.0538,  0.2018],\n",
       "          [-0.3318, -0.1834,  0.3089,  ...,  0.2283,  0.2096,  0.1607],\n",
       "          [-0.2307, -0.0330,  0.2407,  ...,  0.5283, -0.1812,  0.3864],\n",
       "          ...,\n",
       "          [ 0.2279,  0.1810,  0.3491,  ...,  0.2010, -0.0061,  0.3705],\n",
       "          [ 0.3454,  0.3596,  0.4999,  ...,  0.2452, -0.0861,  0.2868],\n",
       "          [ 0.2937,  0.1141, -0.3270,  ...,  0.1992,  0.0176,  0.6156]]]),\n",
       " tensor([[[ 0.3781, -0.0230, -0.0711,  ..., -0.0671, -0.0292,  0.2190],\n",
       "          [ 0.2985, -0.4819, -0.0545,  ..., -0.0742,  0.0046,  0.0328],\n",
       "          [ 0.6118,  0.0857,  0.3057,  ...,  0.5416,  0.2116,  0.5698],\n",
       "          ...,\n",
       "          [ 0.7065, -0.7196,  0.1823,  ...,  0.3742, -0.0597,  0.4548],\n",
       "          [ 0.4179, -0.0896, -0.1182,  ...,  0.2070, -0.1010,  0.2247],\n",
       "          [ 0.7739, -0.0499, -0.3706,  ...,  0.8231, -0.7425,  0.3251]]]),\n",
       " tensor([[[ 0.3282,  0.0326, -0.0273,  ...,  0.0505,  0.1260,  0.3250],\n",
       "          [ 0.1765,  0.0008,  0.1095,  ..., -0.1089,  0.2519,  0.2897],\n",
       "          [ 0.3806,  0.0825, -0.1818,  ...,  0.3915,  0.0637,  0.3621],\n",
       "          ...,\n",
       "          [ 0.2694, -0.3131,  0.2401,  ...,  0.6079, -0.0173,  0.5454],\n",
       "          [ 0.0171,  0.0364, -0.0736,  ..., -0.0369,  0.1625,  0.1167],\n",
       "          [ 0.4849,  0.1978, -0.5824,  ...,  0.6600,  0.2321,  0.6220]]]),\n",
       " tensor([[[ 0.2602, -0.0756,  0.0377,  ..., -0.0562,  0.0304,  0.2421],\n",
       "          [ 0.0561, -0.2730,  0.2218,  ..., -0.0937,  0.0988,  0.0545],\n",
       "          [ 0.2414, -0.1683,  0.3353,  ...,  0.0197,  0.0799, -0.0263],\n",
       "          ...,\n",
       "          [ 0.0235, -0.1104,  0.3288,  ...,  0.6132, -0.2278,  0.4115],\n",
       "          [ 1.1324, -0.0730, -0.4860,  ...,  0.0775,  0.2947,  0.5079],\n",
       "          [ 1.1494, -0.1086, -0.5154,  ...,  0.0868,  0.3053,  0.5952]]]),\n",
       " tensor([[[ 0.3346,  0.0301,  0.0095,  ..., -0.1165,  0.0289,  0.1838],\n",
       "          [-0.1485, -0.4088,  0.0226,  ..., -0.3625,  0.1590,  0.1358],\n",
       "          [ 0.2830, -0.0948,  0.0933,  ..., -0.0461,  0.1753,  0.0138],\n",
       "          ...,\n",
       "          [-0.3153, -0.0394,  0.1090,  ...,  0.1961, -0.1669,  0.2828],\n",
       "          [ 0.0648,  0.0255,  0.1150,  ...,  0.0596, -0.1031,  0.1150],\n",
       "          [ 0.5886,  0.4132, -0.1070,  ...,  0.2666, -0.1579,  0.3346]]]),\n",
       " tensor([[[ 0.3629,  0.0224,  0.0792,  ..., -0.0971,  0.0220,  0.1226],\n",
       "          [ 0.6650,  0.2472,  0.2235,  ..., -0.2865, -0.0920,  0.0172],\n",
       "          [ 0.5746, -0.0770,  0.2103,  ..., -0.0131, -0.0750, -0.1250],\n",
       "          ...,\n",
       "          [-0.1261, -0.0352, -0.1315,  ...,  0.0789,  0.0459, -0.1574],\n",
       "          [ 0.3239, -0.1052,  0.1676,  ..., -0.0255, -0.1495, -0.2838],\n",
       "          [ 0.6792, -0.0210, -0.0815,  ..., -0.2521,  0.2151,  0.0014]]]),\n",
       " tensor([[[ 0.3111,  0.1914, -0.0705,  ..., -0.1903,  0.0032,  0.0770],\n",
       "          [ 0.2423, -0.2481,  0.2239,  ..., -0.4337,  0.0887,  0.0588],\n",
       "          [ 0.1414, -0.0783,  0.3534,  ..., -0.2642, -0.0995,  0.0675],\n",
       "          ...,\n",
       "          [-0.0882,  0.7012,  0.2686,  ...,  0.2063,  0.0894,  0.2981],\n",
       "          [ 0.1701,  0.3758,  0.1459,  ...,  0.1219,  0.0065,  0.3093],\n",
       "          [ 0.6910,  0.3678, -0.3050,  ...,  0.1309, -0.2454,  0.3799]]]),\n",
       " tensor([[[ 0.2591,  0.1398,  0.0400,  ...,  0.0545,  0.1115,  0.2135],\n",
       "          [ 0.2229,  0.0617,  0.0746,  ...,  0.1790,  0.0942, -0.0099],\n",
       "          [ 0.1679, -0.4063, -0.1981,  ...,  0.7205,  0.2992,  0.0282],\n",
       "          ...,\n",
       "          [ 0.3275, -0.2208,  0.3109,  ...,  0.1974,  0.4785,  0.2687],\n",
       "          [ 0.4124,  0.1455,  0.3091,  ...,  0.3489,  0.1110,  0.2123],\n",
       "          [ 0.7693,  0.3005, -0.2976,  ...,  0.3571,  0.5183,  0.5470]]]),\n",
       " tensor([[[ 0.4240,  0.1995, -0.1254,  ..., -0.0784, -0.0532,  0.1169],\n",
       "          [ 0.4091, -0.2629,  0.0134,  ..., -0.0038,  0.0196,  0.0759],\n",
       "          [ 0.7315,  0.1487, -0.0886,  ...,  0.2563, -0.2338,  0.3573],\n",
       "          ...,\n",
       "          [ 0.0491,  0.1156,  0.1265,  ...,  0.3963,  0.3174,  0.3103],\n",
       "          [ 0.3826,  0.3854,  0.1609,  ...,  0.5143, -0.2075,  0.3908],\n",
       "          [ 0.7523,  0.6857, -0.4503,  ...,  0.4869, -0.3578,  0.5994]]]),\n",
       " tensor([[[ 0.3274, -0.0103, -0.0765,  ..., -0.1246, -0.1099,  0.1523],\n",
       "          [ 0.1295, -0.4534,  0.1744,  ..., -0.3044,  0.2529, -0.0215],\n",
       "          [ 0.0932, -0.1982,  0.2272,  ..., -0.0986,  0.1796,  0.0530],\n",
       "          ...,\n",
       "          [ 0.2180,  0.5495,  0.1171,  ...,  0.1767, -0.1370, -0.5351],\n",
       "          [ 0.3704, -0.1089, -0.2386,  ..., -0.1083, -0.1284, -0.1036],\n",
       "          [ 0.6247, -0.1532, -0.3445,  ...,  0.3068, -0.6748,  0.4675]]]),\n",
       " tensor([[[ 0.4320,  0.0785,  0.0049,  ..., -0.1935,  0.0396,  0.1553],\n",
       "          [ 0.4268, -0.2727,  0.3142,  ..., -0.2383,  0.0976,  0.0983],\n",
       "          [ 0.2530,  0.0307,  0.1130,  ...,  0.3082,  0.5560, -0.1747],\n",
       "          ...,\n",
       "          [ 0.4651, -0.2841, -0.0118,  ..., -0.1286, -0.1146, -0.1720],\n",
       "          [ 0.4602,  0.0471,  0.1252,  ...,  0.1667, -0.0775, -0.0271],\n",
       "          [ 0.9062,  0.1324, -0.0480,  ...,  0.1368, -0.0337,  0.3293]]]),\n",
       " tensor([[[ 0.4210, -0.0571, -0.1184,  ..., -0.1169,  0.2336,  0.1296],\n",
       "          [ 0.0512, -0.4841,  0.0535,  ..., -0.0575,  0.4312,  0.2248],\n",
       "          [ 0.6093, -0.0970, -0.2557,  ..., -0.0491,  0.0382, -0.0094],\n",
       "          ...,\n",
       "          [-0.1774, -0.5067,  0.2661,  ..., -0.0966,  0.4555,  0.5491],\n",
       "          [ 0.2529, -0.1230, -0.1146,  ...,  0.1645,  0.1873,  0.4152],\n",
       "          [ 0.5230,  0.0829, -0.3869,  ...,  0.0799,  0.5555,  0.3530]]]),\n",
       " tensor([[[ 0.3717,  0.0474,  0.0334,  ..., -0.1565, -0.0282,  0.2207],\n",
       "          [ 0.1459, -0.0652,  0.2818,  ..., -0.1267,  0.0030,  0.0706],\n",
       "          [ 0.3364,  0.2457,  0.0906,  ...,  0.2569, -0.0294,  0.0525],\n",
       "          ...,\n",
       "          [ 0.1756, -0.6258,  0.7669,  ...,  0.0151, -0.7217,  0.2784],\n",
       "          [ 0.5070, -0.1655,  0.1909,  ...,  0.0199, -0.2520,  0.2903],\n",
       "          [ 0.8157,  0.2919, -0.3490,  ..., -0.0072, -0.3592,  0.1711]]]),\n",
       " tensor([[[ 0.3164, -0.0169, -0.0823,  ..., -0.1929,  0.0472,  0.1286],\n",
       "          [ 0.4617, -0.6183, -0.0189,  ...,  0.0597,  0.2883, -0.1756],\n",
       "          [ 0.1687, -0.3086, -0.0519,  ...,  0.1698,  0.5055, -0.2504],\n",
       "          ...,\n",
       "          [ 0.7628, -0.0235, -0.3177,  ..., -0.1286, -0.7447,  0.7119],\n",
       "          [ 0.4281,  0.0703, -0.0551,  ..., -0.0422, -0.0497,  0.0351],\n",
       "          [ 0.6536,  0.0355, -0.1284,  ..., -0.0460,  0.0376,  0.1635]]]),\n",
       " tensor([[[ 0.2606,  0.1360, -0.0813,  ...,  0.0293,  0.0807,  0.2170],\n",
       "          [ 0.0290, -0.0390, -0.1092,  ...,  0.1610,  0.1232, -0.0814],\n",
       "          [ 0.3154,  0.0170, -0.2550,  ...,  0.4296, -0.0999,  0.2781],\n",
       "          ...,\n",
       "          [ 0.2083,  0.5440, -0.2488,  ...,  0.5589,  0.1898,  0.6233],\n",
       "          [ 0.0122,  0.2919,  0.0899,  ...,  0.3698,  0.0616,  0.2722],\n",
       "          [ 0.2022,  0.4914, -0.5895,  ...,  0.5794,  0.3890,  0.7601]]]),\n",
       " tensor([[[ 0.3479,  0.0423, -0.0113,  ..., -0.1144,  0.0656,  0.2285],\n",
       "          [ 0.0743, -0.4948,  0.1046,  ..., -0.1803,  0.5953, -0.0426],\n",
       "          [ 0.6007, -0.3284,  0.2276,  ...,  0.3054,  0.0934,  0.5623],\n",
       "          ...,\n",
       "          [-0.0394, -0.2571, -0.0149,  ...,  0.1108,  0.0098, -0.1964],\n",
       "          [ 0.3081,  0.1470,  0.2366,  ...,  0.2321,  0.0212,  0.4104],\n",
       "          [ 0.3277,  0.2136, -0.3481,  ...,  0.2025,  0.1972,  0.8259]]]),\n",
       " tensor([[[ 0.3126,  0.1038, -0.1277,  ..., -0.2564,  0.0231,  0.1742],\n",
       "          [-0.0384,  0.0284, -0.3632,  ..., -0.1489,  0.1919,  0.0288],\n",
       "          [ 0.1733,  0.0633, -0.0141,  ..., -0.1493, -0.0223,  0.1040],\n",
       "          ...,\n",
       "          [ 0.1967,  0.5385, -0.2630,  ..., -0.4317, -0.1087,  0.5086],\n",
       "          [ 0.2276,  0.0655, -0.0833,  ..., -0.1374,  0.0532,  0.1890],\n",
       "          [ 0.8882,  0.2363, -0.4810,  ...,  0.1518, -0.2963,  0.1951]]]),\n",
       " tensor([[[ 4.7136e-01,  1.3225e-01,  2.0264e-02,  ..., -3.3382e-02,\n",
       "            1.2576e-01,  6.6181e-02],\n",
       "          [-2.2006e-01,  5.0122e-02, -1.5847e-01,  ..., -5.4095e-01,\n",
       "            2.4568e-01,  3.5104e-01],\n",
       "          [ 1.2159e-01,  1.8093e-01,  1.9709e-02,  ...,  2.4189e-01,\n",
       "            3.6767e-01,  3.5231e-02],\n",
       "          ...,\n",
       "          [ 1.6400e-01, -1.3572e-02,  7.1477e-02,  ...,  1.0073e-01,\n",
       "           -1.4092e-01,  4.0434e-01],\n",
       "          [ 3.3453e-01, -9.8627e-02, -1.2731e-01,  ...,  1.7306e-04,\n",
       "            3.8621e-01,  4.1524e-01],\n",
       "          [ 6.3745e-01,  1.5725e-01, -1.3481e-01,  ...,  1.6835e-01,\n",
       "            1.7048e-01,  3.1160e-01]]]),\n",
       " tensor([[[ 0.3076,  0.0997, -0.0418,  ..., -0.1219, -0.0451,  0.1501],\n",
       "          [ 0.3491, -0.2973,  0.3376,  ..., -0.1346, -0.0490,  0.0986],\n",
       "          [ 0.5139,  0.0977, -0.0678,  ...,  0.3155,  0.0432,  0.0489],\n",
       "          ...,\n",
       "          [ 0.4014, -0.5148, -0.2025,  ..., -0.2115, -0.2775, -0.0504],\n",
       "          [ 0.3948,  0.1396, -0.3095,  ...,  0.2310, -0.0127,  0.1104],\n",
       "          [ 0.4920,  0.1639, -0.4283,  ...,  0.0571, -0.0454,  0.3758]]]),\n",
       " tensor([[[ 0.4362,  0.1238,  0.0296,  ..., -0.0200, -0.0226,  0.0753],\n",
       "          [ 0.3488, -0.0858,  0.0608,  ...,  0.2981,  0.5445,  0.0753],\n",
       "          [ 0.3940,  0.0329, -0.0701,  ...,  0.6944,  0.3512, -0.2040],\n",
       "          ...,\n",
       "          [ 0.4139, -0.0348, -0.0172,  ...,  0.4413, -0.0798,  0.0590],\n",
       "          [ 0.2102,  0.3104,  0.3899,  ...,  0.0989, -0.0050,  0.0136],\n",
       "          [ 1.0671,  0.6892, -0.1079,  ...,  0.5429, -0.1178,  0.4290]]]),\n",
       " tensor([[[ 0.4828,  0.0569, -0.0064,  ..., -0.0243,  0.1040,  0.0949],\n",
       "          [ 0.2228, -0.1307,  0.1920,  ..., -0.0769,  0.1618,  0.0790],\n",
       "          [ 0.5405,  0.1390,  0.1036,  ...,  0.4096,  0.0882,  0.0848],\n",
       "          ...,\n",
       "          [ 0.3271, -0.0173,  0.0083,  ...,  0.1802,  0.1930,  0.2854],\n",
       "          [ 0.4208,  0.2193,  0.1939,  ...,  0.4573, -0.0328,  0.1083],\n",
       "          [ 0.8412,  0.6432, -0.2281,  ...,  0.4423,  0.5165,  0.3582]]]),\n",
       " tensor([[[ 0.3856,  0.0326, -0.0226,  ..., -0.1268,  0.0837,  0.2519],\n",
       "          [ 0.2610, -0.5081,  0.2367,  ..., -0.0434,  0.1156,  0.0533],\n",
       "          [ 0.3161, -0.1960, -0.4285,  ...,  0.3982, -0.2052,  0.1676],\n",
       "          ...,\n",
       "          [ 0.5509, -0.1040,  0.1309,  ..., -0.4711,  0.0197,  0.2751],\n",
       "          [ 0.2925, -0.0276, -0.0150,  ...,  0.0746, -0.0112,  0.0796],\n",
       "          [ 0.6949,  0.2071, -0.5880,  ...,  0.2683,  0.2519,  0.6133]]]),\n",
       " tensor([[[ 0.3201,  0.1345, -0.1217,  ..., -0.0062,  0.1035,  0.2435],\n",
       "          [ 0.5065, -0.6745,  0.5450,  ..., -0.2908,  0.1856,  0.3005],\n",
       "          [ 0.0046,  0.0433,  0.1775,  ..., -0.3158, -0.4260,  0.4604],\n",
       "          ...,\n",
       "          [-0.1258,  0.2186, -0.2681,  ...,  0.1740,  0.0959, -0.0719],\n",
       "          [ 0.2500,  0.2306,  0.0887,  ...,  0.1574, -0.2685,  0.3913],\n",
       "          [ 0.9759,  0.4017, -0.6831,  ...,  0.5013, -0.0629,  0.2279]]]),\n",
       " tensor([[[ 2.6561e-01, -6.8935e-04,  2.3539e-03,  ..., -1.0363e-01,\n",
       "           -4.0790e-02,  1.7874e-01],\n",
       "          [-3.1693e-03, -2.9483e-01,  3.9235e-01,  ..., -2.0618e-01,\n",
       "            8.3605e-02,  1.5853e-01],\n",
       "          [-4.3376e-02,  1.6163e-01,  1.6818e-01,  ...,  1.8408e-01,\n",
       "           -2.9105e-01,  1.8345e-01],\n",
       "          ...,\n",
       "          [-3.5411e-02, -2.3863e-01, -5.1230e-01,  ...,  4.7856e-02,\n",
       "            3.5341e-01,  1.8730e-01],\n",
       "          [ 1.3450e-01, -1.8257e-01,  1.4413e-01,  ..., -3.6552e-01,\n",
       "            9.0497e-02,  2.0560e-01],\n",
       "          [ 8.5652e-01, -2.0388e-01, -1.2830e-01,  ...,  5.3389e-01,\n",
       "           -7.3923e-01,  2.5232e-01]]]),\n",
       " tensor([[[ 0.3307,  0.0604, -0.0276,  ..., -0.1443, -0.0635,  0.1319],\n",
       "          [ 0.4396, -0.7956,  0.2567,  ..., -0.1688,  0.2016,  0.1800],\n",
       "          [ 0.3507, -0.1978, -0.2512,  ...,  0.4549,  0.2585,  0.0269],\n",
       "          ...,\n",
       "          [ 0.1512, -0.0138,  0.3096,  ...,  0.0099, -0.3497,  0.3971],\n",
       "          [ 0.4298,  0.1412,  0.2526,  ...,  0.2190, -0.3304,  0.4344],\n",
       "          [ 0.8905,  0.2058, -0.4666,  ...,  0.2413, -0.6213,  0.2100]]]),\n",
       " tensor([[[ 0.3890,  0.0460,  0.1355,  ..., -0.0715,  0.0918,  0.1258],\n",
       "          [-0.0731,  0.2334,  0.3238,  ..., -0.2852, -0.0922,  0.1481],\n",
       "          [ 0.2175, -0.3257,  0.1836,  ...,  0.1325, -0.1933, -0.0435],\n",
       "          ...,\n",
       "          [ 0.0624, -0.0103,  0.1793,  ..., -0.2188,  0.1482,  0.2838],\n",
       "          [ 0.2289,  0.0535,  0.3258,  ..., -0.1009,  0.0197,  0.0494],\n",
       "          [ 1.0860,  0.4746,  0.0095,  ..., -0.0562,  0.4671,  0.0043]]]),\n",
       " tensor([[[ 0.4127,  0.0880, -0.1140,  ...,  0.0633,  0.0543,  0.1637],\n",
       "          [ 0.0424, -0.0446,  0.0265,  ..., -0.1412,  0.2009,  0.1492],\n",
       "          [ 0.3551,  0.2268,  0.0643,  ...,  0.3415,  0.1285,  0.0152],\n",
       "          ...,\n",
       "          [ 0.1110,  0.3495,  0.0520,  ...,  0.0241,  0.0240,  0.2894],\n",
       "          [ 0.3803,  0.1403, -0.0158,  ...,  0.4418, -0.0822,  0.1365],\n",
       "          [ 0.6589,  0.2353, -0.3443,  ...,  0.7795,  0.0773,  0.5554]]]),\n",
       " tensor([[[ 3.5088e-01, -4.2553e-02,  3.0488e-02,  ..., -2.1685e-01,\n",
       "            7.7999e-02,  1.2031e-01],\n",
       "          [ 6.1531e-01, -3.8744e-01, -1.7239e-01,  ..., -1.9178e-01,\n",
       "            3.1138e-01, -3.6190e-01],\n",
       "          [ 2.8518e-01, -1.5827e-01, -3.2289e-01,  ..., -8.8232e-02,\n",
       "            5.2022e-01,  1.4119e-01],\n",
       "          ...,\n",
       "          [ 5.5454e-01,  3.8644e-04,  2.8543e-01,  ..., -6.5216e-01,\n",
       "            6.8198e-02, -2.6929e-02],\n",
       "          [ 6.9825e-01, -1.1917e-01,  2.0695e-01,  ..., -1.4458e-01,\n",
       "           -2.2512e-02, -3.0520e-02],\n",
       "          [ 7.0782e-01,  2.8301e-01, -1.0526e-01,  ..., -1.6248e-01,\n",
       "           -2.9188e-01,  2.2055e-01]]]),\n",
       " tensor([[[ 0.4095,  0.0863, -0.0745,  ..., -0.0046,  0.0670,  0.1994],\n",
       "          [ 0.0348,  0.1971, -0.1422,  ..., -0.0315, -0.4848,  0.1680],\n",
       "          [ 0.4056, -0.5779,  0.0863,  ...,  0.1253, -0.0009,  0.1323],\n",
       "          ...,\n",
       "          [-0.0555,  0.1310, -0.1387,  ...,  0.8002,  0.3537,  0.4521],\n",
       "          [ 0.3183,  0.1083, -0.0222,  ...,  0.4502, -0.0911,  0.3010],\n",
       "          [ 0.7241,  0.2519, -0.6615,  ...,  0.4454,  0.6692,  0.3497]]]),\n",
       " tensor([[[ 0.1855,  0.0088,  0.0671,  ...,  0.0124,  0.0050,  0.1223],\n",
       "          [-0.1593, -0.1437,  0.2686,  ...,  0.0243,  0.1045,  0.0122],\n",
       "          [ 0.0796,  0.4129,  0.3740,  ...,  0.0868, -0.6160, -0.0446],\n",
       "          ...,\n",
       "          [-0.1249, -0.0847,  0.1720,  ...,  0.1188,  0.2377,  0.2302],\n",
       "          [ 0.0935, -0.0469,  0.1077,  ...,  0.0561,  0.0149, -0.0162],\n",
       "          [ 0.3228, -0.1387, -0.3584,  ..., -0.0618, -0.0904, -0.0051]]]),\n",
       " tensor([[[ 0.3859,  0.1107, -0.0627,  ..., -0.1309, -0.0372,  0.1709],\n",
       "          [ 0.3007, -0.0572,  0.1566,  ..., -0.1352, -0.3560, -0.0128],\n",
       "          [ 0.3965,  0.4525,  0.1400,  ..., -0.0377, -0.4312,  0.4133],\n",
       "          ...,\n",
       "          [ 0.4251,  0.1060,  0.2072,  ...,  0.0652,  0.0809, -0.1128],\n",
       "          [ 0.1689,  0.2115,  0.1907,  ...,  0.3321, -0.1766,  0.3307],\n",
       "          [ 0.7072,  0.0672, -0.3079,  ...,  0.3605, -0.1741,  0.4850]]]),\n",
       " tensor([[[ 0.3251,  0.1063, -0.0373,  ..., -0.0625, -0.0492,  0.0694],\n",
       "          [ 0.3514, -0.2232, -0.3774,  ...,  0.0929,  0.2729, -0.0033],\n",
       "          [ 0.4271, -0.0338, -0.2312,  ..., -0.0201,  0.2450, -0.1643],\n",
       "          ...,\n",
       "          [ 0.0044, -0.1498,  0.1778,  ...,  0.4427, -0.2648,  0.5739],\n",
       "          [ 0.0778, -0.0049,  0.0308,  ...,  0.0462, -0.0733, -0.0754],\n",
       "          [ 0.8931,  0.3419, -0.2875,  ...,  0.1046,  0.2508,  0.3718]]]),\n",
       " tensor([[[ 0.2836, -0.0094, -0.1352,  ..., -0.1466,  0.1449,  0.1022],\n",
       "          [ 0.2725, -0.2496, -0.1339,  ..., -0.4282,  0.2859,  0.1593],\n",
       "          [ 0.1432,  0.1608, -0.2574,  ...,  0.0220,  0.1914,  0.1813],\n",
       "          ...,\n",
       "          [-0.0499, -0.3233, -0.0049,  ..., -0.2199,  0.1916,  0.3771],\n",
       "          [ 0.1847,  0.1310,  0.0058,  ...,  0.0105,  0.1468, -0.0124],\n",
       "          [ 0.6103,  0.6537, -0.6448,  ..., -0.0886,  0.1966,  0.1362]]]),\n",
       " tensor([[[ 0.5035, -0.0375,  0.0519,  ..., -0.0973,  0.0546,  0.1587],\n",
       "          [ 0.0877, -0.1322,  0.1920,  ..., -0.0647, -0.0892, -0.0492],\n",
       "          [ 0.1924, -0.0465,  0.0120,  ...,  0.2729,  0.0325,  0.2990],\n",
       "          ...,\n",
       "          [ 0.2522, -0.3318,  0.1917,  ...,  0.4281, -0.1595,  0.3757],\n",
       "          [ 0.7342,  0.0754,  0.2070,  ...,  0.2840, -0.0724,  0.2228],\n",
       "          [ 1.1025,  0.1610, -0.2519,  ...,  0.3054, -0.0421,  0.3128]]]),\n",
       " tensor([[[ 0.4545,  0.1249, -0.0225,  ..., -0.1056,  0.0709,  0.1486],\n",
       "          [ 0.5512, -0.6330,  0.5488,  ..., -0.2811,  0.1324,  0.2967],\n",
       "          [ 0.5355,  0.0035, -0.2177,  ...,  0.0430, -0.0307,  0.1744],\n",
       "          ...,\n",
       "          [ 0.2367,  0.0817, -0.4588,  ...,  0.0819,  0.1096,  0.2577],\n",
       "          [ 0.5989,  0.2983,  0.0491,  ..., -0.0197, -0.0839,  0.0967],\n",
       "          [ 1.1428,  0.3031, -0.3917,  ...,  0.1813,  0.1054, -0.0056]]]),\n",
       " tensor([[[ 0.3446,  0.0334,  0.0066,  ..., -0.0143,  0.0996,  0.1423],\n",
       "          [ 0.2012, -0.4317,  0.2227,  ..., -0.0177,  0.0551,  0.1372],\n",
       "          [ 0.0331,  0.1379, -0.1805,  ...,  0.6365, -0.4176,  0.1825],\n",
       "          ...,\n",
       "          [ 0.0223, -0.1785,  0.0516,  ...,  0.5508,  0.0650, -0.0539],\n",
       "          [ 0.3218,  0.0417,  0.2121,  ...,  0.1981, -0.0712, -0.1642],\n",
       "          [ 0.4392,  0.1245, -0.1054,  ..., -0.0587,  0.2775,  0.4440]]]),\n",
       " tensor([[[ 0.4306,  0.0028,  0.1192,  ..., -0.1025,  0.0878,  0.0242],\n",
       "          [ 0.0158, -0.0897,  0.3209,  ..., -0.0812,  0.3546,  0.1473],\n",
       "          [ 0.4207,  0.3239,  0.2592,  ...,  0.2109,  0.1429, -0.1015],\n",
       "          ...,\n",
       "          [ 0.2482, -0.3454, -0.1767,  ...,  0.4828,  0.1041, -0.4099],\n",
       "          [ 0.3476,  0.0429,  0.1773,  ...,  0.2322, -0.0114, -0.1967],\n",
       "          [ 0.7847,  0.1762,  0.1064,  ...,  0.0907,  0.0070, -0.0984]]]),\n",
       " tensor([[[ 0.3063,  0.0027, -0.0836,  ..., -0.1217, -0.0746,  0.1715],\n",
       "          [ 0.1488, -0.1381,  0.1766,  ..., -0.2696,  0.0302,  0.1532],\n",
       "          [ 0.2748,  0.0662, -0.2914,  ..., -0.0596, -0.3508,  0.0989],\n",
       "          ...,\n",
       "          [ 0.0047, -0.1713, -0.4580,  ..., -0.1443, -0.3570,  0.2257],\n",
       "          [ 0.3278, -0.1984, -0.0324,  ..., -0.0016, -0.1951, -0.1603],\n",
       "          [ 0.7637,  0.3702, -0.4947,  ...,  0.1652, -0.5187,  0.2022]]]),\n",
       " tensor([[[ 3.9368e-01,  6.2008e-02, -1.2130e-01,  ..., -9.4143e-02,\n",
       "            7.7151e-02,  1.3646e-01],\n",
       "          [ 2.0778e-01, -4.7357e-01,  2.6130e-01,  ...,  9.0883e-02,\n",
       "            2.0855e-02,  1.6435e-01],\n",
       "          [-1.5351e-02,  8.4614e-02, -3.2112e-01,  ...,  4.5863e-01,\n",
       "           -3.9354e-01,  2.0266e-01],\n",
       "          ...,\n",
       "          [ 7.8051e-02,  2.2104e-01, -5.5570e-01,  ...,  7.9430e-01,\n",
       "           -9.8267e-02,  2.3473e-01],\n",
       "          [ 2.9786e-01,  9.4632e-02, -1.5107e-01,  ...,  1.1957e-01,\n",
       "           -3.2473e-02, -7.8278e-04],\n",
       "          [ 4.2497e-01,  4.4345e-01, -7.2124e-01,  ...,  1.3361e-01,\n",
       "            2.4437e-01,  4.5400e-01]]]),\n",
       " tensor([[[ 0.3758,  0.0550,  0.0082,  ..., -0.1044,  0.0809,  0.0561],\n",
       "          [ 0.3367, -0.7100,  0.4269,  ..., -0.2577,  0.2054,  0.1955],\n",
       "          [ 0.2514, -0.0480,  0.0267,  ...,  0.3856,  0.4846, -0.0024],\n",
       "          ...,\n",
       "          [ 0.0047,  0.1088,  0.3530,  ...,  0.4042, -0.1016,  0.2804],\n",
       "          [ 0.3123,  0.1503,  0.0977,  ...,  0.1349, -0.2755,  0.0773],\n",
       "          [ 0.8607,  0.3284, -0.2333,  ...,  0.1437, -0.0334,  0.2404]]]),\n",
       " tensor([[[ 0.3069,  0.0856,  0.0195,  ..., -0.0683,  0.0440,  0.1160],\n",
       "          [ 0.1617, -0.2053,  0.0334,  ...,  0.1065,  0.2084, -0.2271],\n",
       "          [-0.0266, -0.0808, -0.1624,  ...,  0.3381, -0.0741, -0.1031],\n",
       "          ...,\n",
       "          [-0.2453, -0.2967,  0.2115,  ...,  0.2046, -0.1795,  0.4680],\n",
       "          [ 0.1388,  0.0088,  0.0767,  ...,  0.1763,  0.0482,  0.0940],\n",
       "          [ 0.3925,  0.2964, -0.4809,  ...,  0.2004, -0.0774,  0.1885]]]),\n",
       " tensor([[[ 0.4089,  0.1055, -0.0043,  ..., -0.1887,  0.1218,  0.1360],\n",
       "          [ 0.1448,  0.0890,  0.1234,  ..., -0.3034, -0.0227, -0.2489],\n",
       "          [ 0.4062,  0.3721,  0.5973,  ..., -0.1649,  0.4102,  0.0077],\n",
       "          ...,\n",
       "          [ 0.4397,  0.5811,  0.4786,  ...,  0.3490,  0.0440, -0.0253],\n",
       "          [ 0.3794,  0.1590,  0.1806,  ...,  0.2230,  0.1485,  0.2388],\n",
       "          [ 0.8193,  0.3863,  0.1010,  ..., -0.3363,  0.3235,  0.4051]]]),\n",
       " tensor([[[ 0.3055,  0.1255, -0.0969,  ..., -0.1967, -0.0144,  0.0884],\n",
       "          [ 0.3255, -0.1189,  0.1622,  ..., -0.3821,  0.1223, -0.2215],\n",
       "          [ 0.2718,  0.0697, -0.4341,  ..., -0.2860, -0.1959,  0.0115],\n",
       "          ...,\n",
       "          [ 0.0394, -0.2021,  0.0843,  ...,  0.4298, -0.3476,  0.2534],\n",
       "          [ 0.1051,  0.0935,  0.0339,  ...,  0.0982, -0.0596, -0.2557],\n",
       "          [ 0.9337,  0.2080, -0.7314,  ...,  0.2937, -0.3203,  0.3985]]]),\n",
       " tensor([[[ 0.2786,  0.0947,  0.0844,  ..., -0.1114,  0.1227,  0.1297],\n",
       "          [ 0.0979, -0.0422, -0.0312,  ..., -0.0355,  0.3244,  0.0051],\n",
       "          [-0.0149, -0.1741, -0.0058,  ..., -0.1160,  0.0481, -0.0488],\n",
       "          ...,\n",
       "          [-0.2724, -0.2096,  0.2811,  ...,  0.2957,  0.2314,  0.4881],\n",
       "          [ 0.2250,  0.2008,  0.3457,  ...,  0.2770,  0.1178,  0.1201],\n",
       "          [ 0.5655,  0.3017, -0.0208,  ..., -0.2470,  0.7456,  0.6996]]]),\n",
       " tensor([[[ 3.8737e-01,  1.1832e-01, -8.6555e-02,  ..., -2.0554e-02,\n",
       "            2.9318e-02,  1.4946e-01],\n",
       "          [ 7.6000e-04,  3.3146e-01,  3.8582e-01,  ...,  2.2689e-01,\n",
       "           -1.0854e-01,  1.8145e-01],\n",
       "          [ 2.0430e-01, -2.4272e-01, -2.2089e-01,  ...,  1.5118e-02,\n",
       "           -1.1230e-01,  1.4672e-01],\n",
       "          ...,\n",
       "          [-5.1636e-02,  2.2107e-01, -3.6267e-01,  ...,  1.3315e-02,\n",
       "            5.0620e-01, -4.1713e-02],\n",
       "          [ 3.0659e-01,  4.8105e-03, -2.2466e-02,  ...,  1.6888e-01,\n",
       "           -8.5532e-02,  7.9575e-02],\n",
       "          [ 1.1145e+00,  2.8592e-01, -3.3714e-01,  ...,  8.0698e-01,\n",
       "           -1.4788e-01,  4.1604e-01]]]),\n",
       " tensor([[[ 0.2830,  0.1436, -0.0155,  ..., -0.0995, -0.0272,  0.1066],\n",
       "          [-0.0227,  0.1483,  0.2740,  ...,  0.2532, -0.1763,  0.2875],\n",
       "          [-0.0861, -0.1286,  0.0567,  ...,  0.2563,  0.1351,  0.0904],\n",
       "          ...,\n",
       "          [-0.2542,  0.0480,  0.1693,  ...,  0.1099, -0.0314,  0.3455],\n",
       "          [ 0.0120,  0.1989,  0.1764,  ...,  0.2080,  0.2128, -0.1451],\n",
       "          [ 0.8118,  0.4244, -0.3919,  ...,  0.3477, -0.0455,  0.6673]]]),\n",
       " tensor([[[ 0.1991,  0.1410,  0.0237,  ..., -0.0299,  0.0651,  0.1471],\n",
       "          [ 0.1361, -0.2897,  0.3930,  ..., -0.1690, -0.1466, -0.1244],\n",
       "          [ 0.5985, -0.1673, -0.2263,  ..., -0.0933, -0.2071,  0.0476],\n",
       "          ...,\n",
       "          [ 0.0688,  0.1177,  0.3915,  ...,  0.1087,  0.1398,  0.2126],\n",
       "          [ 0.1541,  0.0421,  0.2168,  ...,  0.2345,  0.0494,  0.2208],\n",
       "          [ 0.7807,  0.4014, -0.1084,  ...,  0.2485, -0.0763,  0.3695]]]),\n",
       " tensor([[[ 0.3297,  0.0956, -0.0237,  ..., -0.0229,  0.0651,  0.0449],\n",
       "          [ 0.0260, -0.0597,  0.1969,  ..., -0.1632,  0.0655,  0.0474],\n",
       "          [ 0.1868,  0.2242,  0.0180,  ...,  0.2180, -0.2444,  0.0708],\n",
       "          ...,\n",
       "          [ 0.0823,  0.3930,  0.1900,  ...,  0.0693, -0.2414,  0.1093],\n",
       "          [ 0.3198,  0.1018,  0.1340,  ...,  0.2140, -0.0529, -0.0772],\n",
       "          [ 0.7282,  0.4232, -0.5573,  ...,  0.4715,  0.7748,  0.0133]]]),\n",
       " tensor([[[ 0.2862,  0.0528, -0.0136,  ..., -0.1460, -0.0269,  0.2078],\n",
       "          [ 0.6366, -0.3373,  0.2994,  ..., -0.3142,  0.4807,  0.0624],\n",
       "          [ 0.4030, -0.1248,  0.3094,  ...,  0.0490,  0.2526,  0.1167],\n",
       "          ...,\n",
       "          [ 0.0021, -0.2966,  0.3249,  ...,  0.5777, -0.3738,  0.2469],\n",
       "          [-0.0334, -0.0656,  0.3071,  ..., -0.0061,  0.1452, -0.1436],\n",
       "          [ 0.6805,  0.5891, -0.3402,  ...,  0.1778, -0.3994,  0.6659]]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
